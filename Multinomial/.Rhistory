type = "prob"),
percent = TRUE)
#}
confMatrix.model.rf <- confusionMatrix(pred.model.rf, testset$diagnosis)
#if (tune && confMatrix.model.rf$overall['AccuracyPValue'] < 0.05) {
if (tune) {
print(paste(ntree," and ", mtry))
print(confMatrix.model.rf$overall['Accuracy'])
print(confMatrix.model.rf$overall['AccuracyPValue'])
print(roc.model.rf$auc)
}
var.model.rf <- varImp(model.rf, useModel = TRUE, nonpara = TRUE, scale = TRUE)
var.model.rf <- arrange(var.model.rf, desc(Overall))
}
}
if (!tune) {
return(list(model = model.rf, pred = pred.model.rf, confMatrix = confMatrix.model.rf, var = var.model.rf, roc = roc.model.rf))
}
} else {
model.auto <- rminer::fit(diagnosis ~ .,
data = trainset,
model = "auto",
fdebug = TRUE,
search = list(
search = mparheuristic(
model = c("naive","ctree","cv.glmnet","rpart","kknn","ksvm","lssvm","mlp","mlpe", "randomForest","lda","multinom", "naiveBayes","xgboost"),
task = "class",
inputs = ncol(trainset)-1
),
smethod = "auto",
metric = "AUC",
convex = 0
)
)
pred.model.auto <- predict(model.auto, testset)
roc.model.auto <- multiclass.roc(response = testset$diagnosis,
predictor = predict(model.auto,
newdata = testset[, colnames(testset) != "diagnosis"],
type = "prob"),
percent = TRUE)
var.model.auto <- Importance(model.auto, data = trainset, method = "DSA")
# show leaderboard:
cat("Models  by rank:", model.auto@mpar$LB$model, "\n")
cat("Validation values:", round(model.auto@mpar$LB$eval,4), "\n")
cat("Best model:", model.auto@model, "\n")
cat("AUC", "=", round(mmetric(testset$diagnosis, pred.model.auto, metric="AUC"),2), "\n")
return(list(model = model.auto, pred = pred.model.auto, confMatrix = c(), var = var.model.auto, roc = roc.model.auto))
}
}
# Part 4.3.3.2.4: SVM
learn.features.svm <- perform_learning("SVM", trainset.multinomial, testset.multinomial, svm.kernel = 'laplacedot', svm.cost = 0.1)
perform_learning("RF", trainset.multinomial, testset.multinomial, tune = TRUE)
# Part 4.3.3: Combined Dataset
data.multinomial <- createDataset(dataSource = data.pp, feature = features, probeset = huex.probes, filename = "PCA + GF")
sets <- buildTrainTest(data.multinomial)
trainset.multinomial <- sets$trainset
testset.multinomial <- sets$testset
remove(sets)
perform_learning("RF", trainset.multinomial, testset.multinomial, tune = TRUE)
# Part 5 - Ranking
exportsubdir <- "Step 5 - Ranking"
dir.create(paste(exportdir, exportsubdir, sep = "/"), recursive=TRUE)
# Part 5.1. Build list
ranking <- list(
naive_bayes.gf = rownames(learn.gf.nb$var$importance)[1:20],
naive_bayes.pca = rownames(learn.pca.nb$var$importance)[1:20],
naive_bayes.features = rownames(learn.features.nb$var$importance)[1:20],
knn.gf = rownames(learn.gf.knn$var$importance)[1:20],
knn.pca = rownames(learn.pca.knn$var$importance)[1:20],
knn.features = rownames(learn.features.knn$var$importance)[1:20],
svm.gf = rownames(learn.gf.svm$var$importance)[1:20],
svm.pca = rownames(learn.pca.svm$var$importance)[1:20],
svm.features = rownames(learn.features.svm$var$importance)[1:20],
logistic_regression.gf = gsub("`", "", rownames(learn.gf.log$var), fixed = T)[1:20],
logistic_regression.pca = gsub("`", "", rownames(learn.pca.log$var), fixed = T)[1:20],
logistic_regression.features = gsub("`", "", rownames(learn.pca.log$var), fixed = T)[1:20],
discriminant_analysis.gf = rownames(learn.gf.da$var$importance)[1:20],
discriminant_analysis.pca = rownames(learn.pca.da$var$importance)[1:20],
discriminant_analysis.features = rownames(learn.features.da$var$importance)[1:20],
decision_tree.gf = rownames(learn.gf.dt$var)[1:20],
decision_tree.pca = rownames(learn.pca.dt$var)[1:20],
decision_tree.features = rownames(learn.features.dt$var)[1:20],
random_forest.gf = rownames(learn.gf.rf$var)[1:20],
random_forest.pca = rownames(learn.pca.rf$var)[1:20],
random_forest.features = rownames(learn.features.rf$var)[1:20]
)
# Step 5.2: Export
write.csv(ranking, paste(exportdir, exportsubdir, "Ranking.csv", sep = "/"), row.names = TRUE)
rownames(learn.pca.rf$var)[1:20]
rownames(learn.pca.svm$var$importance)[1:20]
# Part 4.3.2.2.4: SVM (For PCA, SVM tuning had no significant features)
learn.pca.svm <- perform_learning("SVM", trainset.multinomial, testset.multinomial, svm.kernel = 'rbfdot', svm.cost = 1)
# Part 5 - Ranking
exportsubdir <- "Step 5 - Ranking"
dir.create(paste(exportdir, exportsubdir, sep = "/"), recursive=TRUE)
# Part 5.1. Build list
ranking <- list(
naive_bayes.gf = rownames(learn.gf.nb$var$importance)[1:20],
naive_bayes.pca = rownames(learn.pca.nb$var$importance)[1:20],
naive_bayes.features = rownames(learn.features.nb$var$importance)[1:20],
knn.gf = rownames(learn.gf.knn$var$importance)[1:20],
knn.pca = rownames(learn.pca.knn$var$importance)[1:20],
knn.features = rownames(learn.features.knn$var$importance)[1:20],
svm.gf = rownames(learn.gf.svm$var$importance)[1:20],
svm.pca = rownames(learn.pca.svm$var$importance)[1:20],
svm.features = rownames(learn.features.svm$var$importance)[1:20],
logistic_regression.gf = gsub("`", "", rownames(learn.gf.log$var), fixed = T)[1:20],
logistic_regression.pca = gsub("`", "", rownames(learn.pca.log$var), fixed = T)[1:20],
logistic_regression.features = gsub("`", "", rownames(learn.pca.log$var), fixed = T)[1:20],
discriminant_analysis.gf = rownames(learn.gf.da$var$importance)[1:20],
discriminant_analysis.pca = rownames(learn.pca.da$var$importance)[1:20],
discriminant_analysis.features = rownames(learn.features.da$var$importance)[1:20],
decision_tree.gf = rownames(learn.gf.dt$var)[1:20],
decision_tree.pca = rownames(learn.pca.dt$var)[1:20],
decision_tree.features = rownames(learn.features.dt$var)[1:20],
random_forest.gf = rownames(learn.gf.rf$var)[1:20],
random_forest.pca = rownames(learn.pca.rf$var)[1:20],
random_forest.features = rownames(learn.features.rf$var)[1:20]
)
# Step 5.2: Export
write.csv(ranking, paste(exportdir, exportsubdir, "Ranking.csv", sep = "/"), row.names = TRUE)
rownames(learn.pca.svm$var$importance)[1:20]
learn.pca.svm$var
source_url('https://gist.githubusercontent.com/fawda123/6206737/raw/d6f365c283a8cae23fb20892dc223bc5764d50c7/gar_fun.r')
library("downloader")
install.packages("downloader", lib.loc = package_loc)
install.packages("downloader", lib = package_loc)
library("downloader", lib.loc = package_loc)
source_url('https://gist.githubusercontent.com/fawda123/6206737/raw/d6f365c283a8cae23fb20892dc223bc5764d50c7/gar_fun.r')
nrow(features)
nrows(features)
length(features)
#create a pretty color vector for the bar plot
cols<-colorRampPalette(c('lightgreen','lightblue'))(length(features))
#use the function on the model created above
par(mar=c(3,4,1,1),family='serif')
gar.fun('diagnosis', learn.features.svm$model, col=cols, ylab='Rel. importance',ylim=c(-1,1))
gar.fun('diagnosis', learn.features.svm$model)
install.packages(c("downloader", "scales", "reshape"), lib = package_loc)
install.packages(c("downloader", "scales", "reshape"), lib = package_loc)
library("downloader", lib.loc = package_loc); library("scales", lib.loc = package_loc); library("reshape", lib.loc = package_loc)
gar.fun('diagnosis', learn.features.svm$model)
learn.features.svm$var$inputs
learn.features.svm$var$imp
learn.features.svm$var$sresponses
learn.features.svm$var$nclasses
learn.features.svm$var$data
learn.features.svm$var$value
learn.features.svm$var$imp
learn.features.svm$var$interactions
learn.features.svm$var$Llevels
learn.features.svm$var$sresponses
learn.features.svm$var$sresponses[[1:50]]$n
learn.features.svm$var$sresponses[[1]]$n
learn.features.svm$var$sresponses[[2]]$n
learn.features.svm$var$sresponses[[3]]$n
for(i in 1:length(features)) {
print(learn.features.svm$var$sresponses[[i]]$n)
}
features
c(features, learn.features.svm$var$imp)
data.frame(' ' = features, "Overall" = learn.features.svm$var$imp)
data.frame(' ' = features, "Overall" = learn.features.svm$var$imp[-1])
learn.features.svm$var <- data.frame(' ' = features, "Overall" = learn.features.svm$var$imp)
learn.features.svm$var <- data.frame(' ' = features, "Overall" = learn.features.svm$var$imp[-1])
learn.features.svm$var[order(-learn.features.svm$var), ]
order(-learn.features.svm$var)
order(learn.features.svm$var)
order(-learn.features.svm$var$Overall)
learn.features.svm$var <- data.frame(' ' = features, "Overall" = learn.features.svm$var$imp[-1])
learn.features.svm$var[order(-learn.features.svm$var$Overall), ]
learn.features.svm$var <- learn.features.svm$var[order(-learn.features.svm$var$Overall), ]
learn.gf.svm$var <- data.frame(' ' = features, "Overall" = learn.gf.svm$var$imp[-1])
learn.gf.svm$var <- learn.features.svm$var[order(-learn.gf.svm$var$Overall), ]
learn.gf.svm$var <- data.frame(' ' = features.gf, "Overall" = learn.gf.svm$var$imp[-1])
learn.gf.svm$var <- learn.features.svm$var[order(-learn.gf.svm$var$Overall), ]
learn.pca.svm$var <- data.frame(' ' = features.pca, "Overall" = learn.pca.svm$var$imp[-1])
# Part 4.3.2: PCA Dataset
data.multinomial <- createDataset(dataSource = data.pp, feature = features.pca, probeset = huex.probes, filename = "PCA")
sets <- buildTrainTest(data.multinomial)
trainset.multinomial <- sets$trainset
testset.multinomial <- sets$testset
remove(sets)
# Part 4.3.2.2.4: SVM (For PCA, SVM tuning had no significant features)
learn.pca.svm <- perform_learning("SVM", trainset.multinomial, testset.multinomial, svm.kernel = 'rbfdot', svm.cost = 1)
learn.pca.svm$var <- data.frame(' ' = features.pca, "Overall" = learn.pca.svm$var$imp[-1])
learn.pca.svm$var <- learn.features.svm$var[order(-learn.pca.svm$var$Overall), ]
# Part 4.3: Perform ML
# Part 4.3.1: Gene Filtering Dataset
data.multinomial <- createDataset(dataSource = data.pp, feature = features.gf, probeset = huex.probes, filename = "Gene Filtering")
sets <- buildTrainTest(data.multinomial)
trainset.multinomial <- sets$trainset
testset.multinomial <- sets$testset
remove(sets)
# Part 4.3.1.2.4: SVM
learn.gf.svm <- perform_learning("SVM", trainset.multinomial, testset.multinomial, svm.kernel = 'laplacedot', svm.cost = 100)
learn.gf.svm$var <- data.frame('feature' = features.gf, "Overall" = learn.gf.svm$var$imp[-1])
learn.gf.svm$var <- learn.features.svm$var[order(-learn.gf.svm$var$Overall), ]
# Part 4.3.2: PCA Dataset
data.multinomial <- createDataset(dataSource = data.pp, feature = features.pca, probeset = huex.probes, filename = "PCA")
sets <- buildTrainTest(data.multinomial)
trainset.multinomial <- sets$trainset
testset.multinomial <- sets$testset
remove(sets)
# Part 4.3.2.2.4: SVM (For PCA, SVM tuning had no significant features)
learn.pca.svm <- perform_learning("SVM", trainset.multinomial, testset.multinomial, svm.kernel = 'rbfdot', svm.cost = 1)
learn.pca.svm$var <- data.frame('feature' = features.pca, "Overall" = learn.pca.svm$var$imp[-1])
learn.pca.svm$var <- learn.features.svm$var[order(-learn.pca.svm$var$Overall), ]
# Part 4.3.3: Combined Dataset
data.multinomial <- createDataset(dataSource = data.pp, feature = features, probeset = huex.probes, filename = "PCA + GF")
sets <- buildTrainTest(data.multinomial)
trainset.multinomial <- sets$trainset
testset.multinomial <- sets$testset
remove(sets)
# Part 4.3.3.2.4: SVM
learn.features.svm <- perform_learning("SVM", trainset.multinomial, testset.multinomial, svm.kernel = 'laplacedot', svm.cost = 0.1)
learn.features.svm$var <- data.frame('feature' = features, "Overall" = learn.features.svm$var$imp[-1])
learn.features.svm$var <- learn.features.svm$var[order(-learn.features.svm$var$Overall), ]
# Part 5 - Ranking
exportsubdir <- "Step 5 - Ranking"
dir.create(paste(exportdir, exportsubdir, sep = "/"), recursive=TRUE)
# Part 5.1. Build list
ranking <- list(
naive_bayes.gf = rownames(learn.gf.nb$var$importance)[1:20],
naive_bayes.pca = rownames(learn.pca.nb$var$importance)[1:20],
naive_bayes.features = rownames(learn.features.nb$var$importance)[1:20],
knn.gf = rownames(learn.gf.knn$var$importance)[1:20],
knn.pca = rownames(learn.pca.knn$var$importance)[1:20],
knn.features = rownames(learn.features.knn$var$importance)[1:20],
svm.gf = rownames(learn.gf.svm$var$feature)[1:20],
svm.pca = rownames(learn.pca.svm$var$feature)[1:20],
svm.features = rownames(learn.features.svm$var$feature)[1:20],
logistic_regression.gf = gsub("`", "", rownames(learn.gf.log$var), fixed = T)[1:20],
logistic_regression.pca = gsub("`", "", rownames(learn.pca.log$var), fixed = T)[1:20],
logistic_regression.features = gsub("`", "", rownames(learn.pca.log$var), fixed = T)[1:20],
discriminant_analysis.gf = rownames(learn.gf.da$var$importance)[1:20],
discriminant_analysis.pca = rownames(learn.pca.da$var$importance)[1:20],
discriminant_analysis.features = rownames(learn.features.da$var$importance)[1:20],
decision_tree.gf = rownames(learn.gf.dt$var)[1:20],
decision_tree.pca = rownames(learn.pca.dt$var)[1:20],
decision_tree.features = rownames(learn.features.dt$var)[1:20],
random_forest.gf = rownames(learn.gf.rf$var)[1:20],
random_forest.pca = rownames(learn.pca.rf$var)[1:20],
random_forest.features = rownames(learn.features.rf$var)[1:20]
)
# Step 5.2: Export
write.csv(ranking, paste(exportdir, exportsubdir, "Ranking.csv", sep = "/"), row.names = TRUE)
rownames(learn.gf.svm$var$feature)
learn.gf.svm$var$feature
# Part 4.3.3.2.4: SVM
learn.features.svm <- perform_learning("SVM", trainset.multinomial, testset.multinomial, svm.kernel = 'laplacedot', svm.cost = 0.1)
learn.features.svm$var <- data.frame('feature' = features, "Overall" = learn.features.svm$var$imp[-1])
learn.features.svm$var
learn.features.svm$var$feature
learn.gf.svm$var$feature[1:20]
learn.gpca.svm$var$feature[1:20]
learn.pca.svm$var$feature[1:20]
learn.feature.svm$var$feature[1:20]
learn.features.svm$var$feature[1:20]
# Part 4.3: Perform ML
# Part 4.3.1: Gene Filtering Dataset
data.multinomial <- createDataset(dataSource = data.pp, feature = features.gf, probeset = huex.probes, filename = "Gene Filtering")
sets <- buildTrainTest(data.multinomial)
trainset.multinomial <- sets$trainset
testset.multinomial <- sets$testset
remove(sets)
# Part 4.3.1.2.4: SVM
learn.gf.svm <- perform_learning("SVM", trainset.multinomial, testset.multinomial, svm.kernel = 'laplacedot', svm.cost = 100)
learn.gf.svm$var <- data.frame('feature' = features.gf, "Overall" = learn.gf.svm$var$imp[-1])
learn.gf.svm$var <- learn.gf.svm$var[order(-learn.gf.svm$var$Overall), ]
# Part 4.3.2: PCA Dataset
data.multinomial <- createDataset(dataSource = data.pp, feature = features.pca, probeset = huex.probes, filename = "PCA")
sets <- buildTrainTest(data.multinomial)
trainset.multinomial <- sets$trainset
testset.multinomial <- sets$testset
remove(sets)
# Part 4.3.2.2.4: SVM (For PCA, SVM tuning had no significant features)
learn.pca.svm <- perform_learning("SVM", trainset.multinomial, testset.multinomial, svm.kernel = 'rbfdot', svm.cost = 1)
learn.pca.svm$var <- data.frame('feature' = features.pca, "Overall" = learn.pca.svm$var$imp[-1])
learn.pca.svm$var <- learn.pca.svm$var[order(-learn.pca.svm$var$Overall), ]
# Part 5 - Ranking
exportsubdir <- "Step 5 - Ranking"
dir.create(paste(exportdir, exportsubdir, sep = "/"), recursive=TRUE)
# Part 5.1. Build list
ranking <- list(
naive_bayes.gf = rownames(learn.gf.nb$var$importance)[1:20],
naive_bayes.pca = rownames(learn.pca.nb$var$importance)[1:20],
naive_bayes.features = rownames(learn.features.nb$var$importance)[1:20],
knn.gf = rownames(learn.gf.knn$var$importance)[1:20],
knn.pca = rownames(learn.pca.knn$var$importance)[1:20],
knn.features = rownames(learn.features.knn$var$importance)[1:20],
svm.gf = learn.gf.svm$var$feature[1:20],
svm.pca =learn.pca.svm$var$feature[1:20],
svm.features = learn.features.svm$var$feature[1:20],
logistic_regression.gf = gsub("`", "", rownames(learn.gf.log$var), fixed = T)[1:20],
logistic_regression.pca = gsub("`", "", rownames(learn.pca.log$var), fixed = T)[1:20],
logistic_regression.features = gsub("`", "", rownames(learn.pca.log$var), fixed = T)[1:20],
discriminant_analysis.gf = rownames(learn.gf.da$var$importance)[1:20],
discriminant_analysis.pca = rownames(learn.pca.da$var$importance)[1:20],
discriminant_analysis.features = rownames(learn.features.da$var$importance)[1:20],
decision_tree.gf = rownames(learn.gf.dt$var)[1:20],
decision_tree.pca = rownames(learn.pca.dt$var)[1:20],
decision_tree.features = rownames(learn.features.dt$var)[1:20],
random_forest.gf = rownames(learn.gf.rf$var)[1:20],
random_forest.pca = rownames(learn.pca.rf$var)[1:20],
random_forest.features = rownames(learn.features.rf$var)[1:20]
)
# Step 5.2: Export
write.csv(ranking, paste(exportdir, exportsubdir, "Ranking.csv", sep = "/"), row.names = TRUE)
# Step 5.3: Get data from annotation
write.csv(huex.probes[, which(huex.probes$probeset_id %in% features)], paste(exportdir, exportsubdir, "Annotated Features.csv", sep = "/"), row.names = TRUE)
save.image("E:/jmcco/OneDrive - University of the Philippines/School/AY 2022-2023/2nd Sem/HI 299 Research Methods in Health Informatics/HI 299 Project/DLPFC-Gene-Expression/Multinomial/.RData")
# Step 5.3: Get data from annotation
write.csv(huex.probes[, which(huex.probes$probeset_id %in% features)], paste(exportdir, exportsubdir, "Annotated Features.csv", sep = "/"), row.names = TRUE)
features
nrow(features)
length(features)
trainset.multinomial
trainset.multinomial$diagnosis
length(trainset.multinomial$diagnosis)
help(trainControl)
length(features.gf)
length(features.pca)
length(features)
# Part 1: Setting up
exportsubdir <- "Part 1 - Setup"
# Part 1.1: Sets the location of the data to be used and where the packages should be put
datadir <- "E:/jmcco/Downloads/BNF 300.2 Data/GSE208338_RAW/"
#probedir <- "E:/jmcco/Downloads/BNF 300.2 Data/Affymetrix_HuEx/"
probedir <- "E:/jmcco/Downloads/BNF 300.2 Data/HuEx-1_0-st-v2-na36-hg19 Probeset/"
setwd(datadir)
exportdir <- paste(dirname(rstudioapi::getSourceEditorContext()$path), "/Export", sep = "")
package_loc <- paste(datadir, "lib", sep = "")
library(BiocManager, lib.loc = package_loc)
library(Biobase, lib.loc = package_loc); library(BiocSingular, lib.loc = package_loc); library(tzdb, lib.loc = package_loc); library(vroom, lib.loc = package_loc); library(readr, lib.loc = package_loc)
library(S4Vectors, lib.loc = package_loc); library(IRanges, lib.loc = package_loc); library(XVector, lib.loc = package_loc); library(GenomeInfoDb, lib.loc = package_loc); library(Biostrings, lib.loc = package_loc)
library(withr, lib = package_loc); library(backports, lib.loc = package_loc); library(ggcorrplot, lib.loc = package_loc); library(ggpubr, lib = package_loc);
library(BiocGenerics, lib.loc = package_loc); library(dplyr, lib.loc = package_loc);
library(oligoClasses, lib.loc = package_loc); library(memoise, lib.loc = package_loc); library(pd.huex.1.0.st.v2, lib.loc = package_loc); library(oligo, lib.loc = package_loc, attach.required = TRUE)
# Part 2: Exploration
library(withr, lib = package_loc); library(ggplot2, lib = package_loc);
library(genefilter, lib = package_loc); library(memoise, lib = package_loc); library(limma, lib = package_loc); library(labeling, lib = package_loc); library(farver, lib = package_loc); library(ggvenn, lib = package_loc)
library(multtest, lib = package_loc);  library(pkgconfig, lib = package_loc);  library(GO.db, lib = package_loc); library(annaffy, lib = package_loc)
# Part 3: Dimension Reduction
library(locfit, lib.loc = package_loc)
library(corrr, lib.loc = package_loc); library(idm, lib.loc = package_loc); library(irlba, lib.loc = package_loc)
library(PCAtools, lib.loc = package_loc); library(RMTstat, lib.loc = package_loc); library(rappdirs, lib.loc = package_loc); library(biomaRt, lib.loc = package_loc); library(cowplot, lib.loc = package_loc); library(ggplotify, lib.loc = package_loc)
library(pROC, lib.loc = package_loc); library(withr, lib.loc = package_loc);
library(EFA.dimensions, lib.loc = package_loc)
library(corrplot, lib.loc = package_loc); library(factoextra, lib.loc = package_loc); library(car, lib.loc = package_loc)
library(jsonlite, lib.loc = package_loc); library(backports, lib.loc = package_loc); library(Hmisc, lib.loc = package_loc);
# Part 4: Analysis
library(e1071, lib.loc = package_loc); library(naivebayes, lib.loc = package_loc)
library(class, lib.loc = package_loc); library(gmodels, lib.loc = package_loc)
library(parallel, lib.loc = package_loc); library(doParallel, lib.loc = package_loc)
library(rpart, lib.loc = package_loc); library(rpart.plot, lib.loc = package_loc)
library(nnet, lib.loc = package_loc); library(rminer, lib.loc = package_loc)
library(randomForest, lib.loc = package_loc)
library(MASS, lib.loc = package_loc); library(Metrics, lib.loc = package_loc);
library(caret, lib.loc = package_loc);
# Part 3: Data Preparation
exportsubdir <- "Step 3 - Dim Redux"
dir.create(paste(exportdir, exportsubdir, sep = "/"), recursive=TRUE)
# Step 3.3: Combine the features from Gene Filtering and PCA
features <- unique(c(features.gf, features.pca))
length(features)
# Step 3.3.1: Look for perfect collinearity
data.features <- as.matrix(exprs(data.pp)[features, ])
show_perfect_collinearity(data.features)
# Step 3.3.2: Since there is no perfect collinearity, proceed
data.features <- as.matrix(exprs(data.pp)[features, ])
data.features <- rbind(data.features, diagnosis.binom)
data.features <- as.data.frame(t(data.features))
model.features <- lm(diagnosis.binom ~ ., data = data.features)
# Part 3.3.3: Get only the factors with no to moderate collinearity (VIF <= 5)
features <- names(vif(model.features)[vif(model.features) <= 5])
features <- gsub("`", "", features, fixed = T)
remove(data.features, model.features)
# Part 3.3.4: Put to HTML the names of the features
atab <- aafTableAnn(features.pca, "pd.huex.1.0.st.v2", aaf.handler()[c(1:3,8:9,11:13)])
saveHTML(atab, file=paste(datadir, "../Export/GF + PCA Probe Names.html", sep = ""))
# Part 3.3.5: Create a correlation matrix across each features
features.corr <- rcorr(as.matrix(t(exprs(data.pp)[features, ])))
corrplot(features.corr$r)
# Step 3.3.6: create Venn diagram and display all sets
ggvenn(list('Gene Filter' = features.gf,
'PCA' = features.pca
),
digits = 2
)
# Step 3.3.7: Export
write.csv(features, paste(exportdir, exportsubdir, "Features (GF + PCA).csv", sep = "/"), row.names = TRUE)
length(features)
learn.gf.nb$var$importance
# Part 5 - Ranking
exportsubdir <- "Step 5 - Ranking"
dir.create(paste(exportdir, exportsubdir, sep = "/"), recursive=TRUE)
# Step 5.1. Features in GF
# Step 5.1.1. Build list
ranking.gf <- list(
naive_bayes.gf = rownames(learn.gf.nb$var$importance)[1:20],
knn.gf = rownames(learn.gf.knn$var$importance)[1:20],
svm.gf = learn.gf.svm$var$feature[1:20],
logistic_regression.gf = gsub("`", "", rownames(learn.gf.log$var), fixed = T)[1:20],
discriminant_analysis.gf = rownames(learn.gf.da$var$importance)[1:20],
decision_tree.gf = rownames(learn.gf.dt$var)[1:20],
random_forest.gf = rownames(learn.gf.rf$var)[1:20],
)
learn.gf.rf$var
# Step 5.1. Features in GF
# Step 5.1.1. Build list
ranking.gf <- list(
naive_bayes.gf = rownames(learn.gf.nb$var$importance)[1:20],
knn.gf = rownames(learn.gf.knn$var$importance)[1:20],
svm.gf = learn.gf.svm$var$feature[1:20],
logistic_regression.gf = gsub("`", "", rownames(learn.gf.log$var), fixed = T)[1:20],
discriminant_analysis.gf = rownames(learn.gf.da$var$importance)[1:20],
decision_tree.gf = rownames(learn.gf.dt$var)[1:20],
random_forest.gf = rownames(learn.gf.rf$var)[1:20]
)
# Step 5.1. Features in GF
# Step 5.1.1. Build list
ranking.gf <- list(
naive_bayes.gf = rownames(learn.gf.nb$var$importance),
knn.gf = rownames(learn.gf.knn$var$importance),
svm.gf = learn.gf.svm$var$feature,
logistic_regression.gf = gsub("`", "", rownames(learn.gf.log$var), fixed = T),
discriminant_analysis.gf = rownames(learn.gf.da$var$importance),
decision_tree.gf = rownames(learn.gf.dt$var),
random_forest.gf = rownames(learn.gf.rf$var)
)
# Step 5.1.2: Export
write.csv(ranking.gf, paste(exportdir, exportsubdir, "Ranking (GF).csv", sep = "/"), row.names = TRUE)
ranking.gf
learn.gf.nb$var$importance
learn.gf.nb$var
learn.gf.nb$var$model
learn.gf.log$var
learn.gf.da$var$importance
learn.gf.da$var
rownames(learn.gf.da$var)
learn.gf.da$var
View(learn.gf.da$var)
learn.gf.nb$var
learn.gf.nb$var
nrow(learn.gf.nb$var)
learn.gf.nb$var
learn.gf.nb$var['X1']
help(varImp)
learn.features.auto$var
rownames(learn.features.auto$var) <- 1:nrow(learn.features.auto$var)
learn.features.auto$var
rownames(learn.features.svm$var) <- 1:nrow(learn.features.svm$var)
learn.features.svm$var
rownames(learn.features.log$var) <- 1:nrow(learn.features.log$var)
learn.features.log$var
rownames(learn.features.dt$var) <- 1:nrow(learn.features.dt$var)
learn.features.dt$var
save.image("E:/jmcco/OneDrive - University of the Philippines/School/AY 2022-2023/2nd Sem/HI 299 Research Methods in Health Informatics/HI 299 Project/DLPFC-Gene-Expression/Multinomial/.RData")
features
learn.features.svm$var[order(learn.features.svm$var$feature),]
learn.features.log$var[order(learn.features.log$var$feature),]
learn.features.dt$var[order(learn.features.dt$var$feature),]
learn.features.rf$var[order(learn.features.rf$var$feature),]
learn.features.rf$var
cbind(learn.features.rf$var, c("ranking" = 1:nrow(learn.features.rf$var))
)
cbind(learn.features.rf$var, "feature" = rownames(learn.features.rf$var))
a <- cbind(learn.features.rf$var, "feature" = rownames(learn.features.rf$var))
rownames(a) <- 1:nrow(a)
a
a[order(a$feature), ]
learn.features.log$roc
learn.features.log$roc$auc
data.pp
data.pp
# Part 1: Setting up
exportsubdir <- "Part 1 - Setup"
# Part 1.1: Sets the location of the data to be used and where the packages should be put
datadir <- "E:/jmcco/Downloads/BNF 300.2 Data/GSE208338_RAW/"
#probedir <- "E:/jmcco/Downloads/BNF 300.2 Data/Affymetrix_HuEx/"
probedir <- "E:/jmcco/Downloads/BNF 300.2 Data/HuEx-1_0-st-v2-na36-hg19 Probeset/"
setwd(datadir)
exportdir <- paste(dirname(rstudioapi::getSourceEditorContext()$path), "/Export", sep = "")
package_loc <- paste(datadir, "lib", sep = "")
library(BiocManager, lib.loc = package_loc)
library(Biobase, lib.loc = package_loc); library(BiocSingular, lib.loc = package_loc); library(tzdb, lib.loc = package_loc); library(vroom, lib.loc = package_loc); library(readr, lib.loc = package_loc)
library(S4Vectors, lib.loc = package_loc); library(IRanges, lib.loc = package_loc); library(XVector, lib.loc = package_loc); library(GenomeInfoDb, lib.loc = package_loc); library(Biostrings, lib.loc = package_loc)
library(withr, lib = package_loc); library(backports, lib.loc = package_loc); library(ggcorrplot, lib.loc = package_loc); library(ggpubr, lib = package_loc);
library(BiocGenerics, lib.loc = package_loc); library(dplyr, lib.loc = package_loc);
library(oligoClasses, lib.loc = package_loc); library(memoise, lib.loc = package_loc); library(pd.huex.1.0.st.v2, lib.loc = package_loc); library(oligo, lib.loc = package_loc, attach.required = TRUE)
# Part 2: Exploration
library(withr, lib = package_loc); library(ggplot2, lib = package_loc);
library(genefilter, lib = package_loc); library(memoise, lib = package_loc); library(limma, lib = package_loc); library(labeling, lib = package_loc); library(farver, lib = package_loc); library(ggvenn, lib = package_loc)
library(multtest, lib = package_loc);  library(pkgconfig, lib = package_loc);  library(GO.db, lib = package_loc); library(annaffy, lib = package_loc)
# Part 3: Dimension Reduction
library(locfit, lib.loc = package_loc)
library(corrr, lib.loc = package_loc); library(idm, lib.loc = package_loc); library(irlba, lib.loc = package_loc)
library(PCAtools, lib.loc = package_loc); library(RMTstat, lib.loc = package_loc); library(rappdirs, lib.loc = package_loc); library(biomaRt, lib.loc = package_loc); library(cowplot, lib.loc = package_loc); library(ggplotify, lib.loc = package_loc)
library(pROC, lib.loc = package_loc); library(withr, lib.loc = package_loc);
library(EFA.dimensions, lib.loc = package_loc)
library(corrplot, lib.loc = package_loc); library(factoextra, lib.loc = package_loc); library(car, lib.loc = package_loc)
library(jsonlite, lib.loc = package_loc); library(backports, lib.loc = package_loc); library(Hmisc, lib.loc = package_loc);
# Part 4: Analysis
library(e1071, lib.loc = package_loc); library(naivebayes, lib.loc = package_loc)
library(class, lib.loc = package_loc); library(gmodels, lib.loc = package_loc)
library(parallel, lib.loc = package_loc); library(doParallel, lib.loc = package_loc)
library(rpart, lib.loc = package_loc); library(rpart.plot, lib.loc = package_loc)
library(nnet, lib.loc = package_loc); library(rminer, lib.loc = package_loc)
library(randomForest, lib.loc = package_loc)
library(MASS, lib.loc = package_loc); library(Metrics, lib.loc = package_loc);
library(caret, lib.loc = package_loc);
