repeats = 10)
model.dt <- train(x = trainset[, colnames(trainset) != "diagnosis"],
y = trainset$diagnosis,
method = "rpart2",
trControl = trControl.dt
)
pred.model.dt <- predict(model.dt, newdata = testset)
confMatrix.model.dt <- confusionMatrix(pred.model.dt, testset$diagnosis)
} else if (method == "SVM") {
# Part 4.4: SVM
# For SVM and random forest, cut the dataset to 10% of the dataset to make processing quicker
#trainset.cut <- trainset[sample(x = 1:nrow(trainset), size = nrow(trainset) * .10, replace = TRUE), colnames(trainset)]
#trainset.cut <- upSample(x = trainset.cut[, colnames(trainset.cut) %ni% "ADDEPEV3"], yname = "ADDEPEV3", y = trainset.cut$ADDEPEV3)
if (tune) {
kernels <- c("rbfdot", "polydot", "tanhdot", "vanilladot", "laplacedot", "besseldot", "anovadot", "splinedot")
costs <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
} else {
kernels <- c(svm.kernel)
costs <- c(svm.cost)
}
for (kernel in kernels) {
for (cost in costs) {
print(paste(kernel," @ ", cost))
model.svm <- rminer::fit(diagnosis ~ .,
data = trainset,
model = "svm",
kernel = kernel,
kpar = "automatic",
C = cost,
task = "class"
)
pred.model.svm <- predict(model.svm, newdata = testset)
confMatrix.model.svm <- confusionMatrix(pred.model.svm, testset$diagnosis)
print(confMatrix.model.svm)
var.model.svm <- Importance(model.svm, data = trainset)
}
}
} else if (method == "LOG") {
model.logit <- multinom(diagnosis ~ .,
data = trainset)
pred.model.logit <- predict(model.logit, newdata = testset[, colnames(testset) != "diagnosis"], type = "class")
confMatrix.model.logit <- confusionMatrix(pred.model.logit, testset$diagnosis)
var.model.logit <- varImp(model.logit, useModel = TRUE, nonpara = TRUE, scale = TRUE)
summary(model.logit)
View(cbind("coeff" = coef(model.logit), "odds ratio" = (exp(coef(model.logit)) - 1) * 100)) # Odds ratio
} else if (method == "DA") {
# Part 4.6: Discriminant Analysis
} else if (method == "RF") {
# Part 4.7: Random Forest
set.seed(100)
if (tune) {
mtries <- sort.int(sample(ncol(trainset)-1, 5))
ntrees <- c(201, 501, 1501, 2501, 3501)
} else {
mtries <- c(rf.mtry)
ntrees <- c(rf.ntree)
}
for(ntree in ntrees) {
for(mtry in mtries) {
model.rf <- randomForest(x = trainset[, colnames(trainset) != "diagnosis"],
y = trainset$diagnosis,
ntree = ntree,
mtry = mtry
)
pred.model.rf <- predict(model.rf, newdata = testset)
confMatrix.model.rf <- confusionMatrix(pred.model.rf, testset$diagnosis)
}
}
} else {
print('No such methodology')
}
}
perform_learning("RF", trainset.multinomial[, features.selected], testset.multinomial[, features.selected], tune = TRUE)
perform_learning("RF", trainset.multinomial[, features.selected], testset.multinomial[, features.selected], tune = TRUE)
# Part 4.2: Create a function for each ML
perform_learning <- function(method, trainset, testset,
svm.kernel = NULL,
svm.cost = NULL,
rf.ntree = NULL,
rf.mtry = NULL,
export.filename = NULL,
tune = FALSE) {
if (method == "NB") {
# Part 4.1: Naive Bayes
set.seed(100)
model.nb <- train(x = trainset[, colnames(trainset) != "diagnosis"],
y = trainset$diagnosis,
method = "naive_bayes",
trControl = trainControl(method='cv', number=10)
)
pred.model.nb <- predict(model.nb, newdata = testset[, colnames(trainset) != "diagnosis"])
confMatrix.model.nb <- confusionMatrix(pred.model.nb, testset$diagnosis)
var.model.nb <- varImp(model.nb, useModel = TRUE, nonpara = TRUE, scale = TRUE)
} else if (method == "KNN") {
# Part 4.2: K Nearest Neighbors
set.seed(100)
trControl.knn <- trainControl(method='repeatedcv', number = 3, allowParallel = TRUE)
trainset.preprocessed <- preProcess(trainset[, colnames(trainset) != "diagnosis"])
model.knn <- train(x = trainset[, colnames(trainset) != "diagnosis"],
y = trainset$diagnosis,
method = "knn",
trControl = trControl.knn,
tuneLength = 20
)
pred.model.knn <- predict(model.knn, newdata = testset)
confMatrix.model.knn <- confusionMatrix(pred.model.knn, testset$diagnosis)
var.model.knn <- varImp(model.knn, useModel = TRUE, nonpara = TRUE, scale = TRUE)
} else if (method == "DT") {
# Part 4.3: Decision Tree
# Part 4.3.1: Using rpart
model.dt <- rpart(formula = diagnosis ~ .,
data = trainset,
method = "class",
control = rpart.control(minsplit=2, minbucket = 1, cp = 0.001)
)
var.model.dt<- varImp(model.dt, useModel = TRUE, nonpara = TRUE, scale = TRUE)
rpart.plot(model.dt)
pdf(export.filename)
prp(model.dt, extra=104)
dev.off()
# Part 4.3.2: Using train
## 10-fold CV
## repeated ten times
trControl.dt <- trainControl(
method = "repeatedcv",
number = 5,
repeats = 10)
model.dt <- train(x = trainset[, colnames(trainset) != "diagnosis"],
y = trainset$diagnosis,
method = "rpart2",
trControl = trControl.dt
)
pred.model.dt <- predict(model.dt, newdata = testset)
confMatrix.model.dt <- confusionMatrix(pred.model.dt, testset$diagnosis)
} else if (method == "SVM") {
# Part 4.4: SVM
# For SVM and random forest, cut the dataset to 10% of the dataset to make processing quicker
#trainset.cut <- trainset[sample(x = 1:nrow(trainset), size = nrow(trainset) * .10, replace = TRUE), colnames(trainset)]
#trainset.cut <- upSample(x = trainset.cut[, colnames(trainset.cut) %ni% "ADDEPEV3"], yname = "ADDEPEV3", y = trainset.cut$ADDEPEV3)
if (tune) {
kernels <- c("rbfdot", "polydot", "tanhdot", "vanilladot", "laplacedot", "besseldot", "anovadot", "splinedot")
costs <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
} else {
kernels <- c(svm.kernel)
costs <- c(svm.cost)
}
for (kernel in kernels) {
for (cost in costs) {
print(paste(kernel," @ ", cost))
model.svm <- rminer::fit(diagnosis ~ .,
data = trainset,
model = "svm",
kernel = kernel,
kpar = "automatic",
C = cost,
task = "class"
)
pred.model.svm <- predict(model.svm, newdata = testset)
confMatrix.model.svm <- confusionMatrix(pred.model.svm, testset$diagnosis)
print(confMatrix.model.svm)
var.model.svm <- Importance(model.svm, data = trainset)
}
}
} else if (method == "LOG") {
model.logit <- multinom(diagnosis ~ .,
data = trainset)
pred.model.logit <- predict(model.logit, newdata = testset[, colnames(testset) != "diagnosis"], type = "class")
confMatrix.model.logit <- confusionMatrix(pred.model.logit, testset$diagnosis)
var.model.logit <- varImp(model.logit, useModel = TRUE, nonpara = TRUE, scale = TRUE)
summary(model.logit)
View(cbind("coeff" = coef(model.logit), "odds ratio" = (exp(coef(model.logit)) - 1) * 100)) # Odds ratio
} else if (method == "DA") {
# Part 4.6: Discriminant Analysis
} else if (method == "RF") {
# Part 4.7: Random Forest
set.seed(100)
if (tune) {
mtries <- sort.int(sample(ncol(trainset)-1, 5))
ntrees <- c(201, 501, 1501, 2501, 3501)
} else {
mtries <- c(rf.mtry)
ntrees <- c(rf.ntree)
}
for(ntree in ntrees) {
for(mtry in mtries) {
print(paste(ntree," and ", mtry))
model.rf <- randomForest(x = trainset[, colnames(trainset) != "diagnosis"],
y = trainset$diagnosis,
ntree = ntree,
mtry = mtry
)
pred.model.rf <- predict(model.rf, newdata = testset)
confMatrix.model.rf <- confusionMatrix(pred.model.rf, testset$diagnosis)
print(confMatrix.model.rf)
var.model.rf <- varImp(model.rf, useModel = TRUE, nonpara = TRUE, scale = TRUE)
}
}
} else {
print('No such methodology')
}
}
perform_learning("RF", trainset.multinomial[, features.selected], testset.multinomial[, features.selected], tune = TRUE)
library(Biobase, lib.loc = package_loc); library(BiocSingular, lib.loc = package_loc); library(tzdb, lib.loc = package_loc); library(vroom, lib.loc = package_loc); library(readr, lib.loc = package_loc)
library(S4Vectors, lib.loc = package_loc); library(IRanges, lib.loc = package_loc); library(XVector, lib.loc = package_loc); library(GenomeInfoDb, lib.loc = package_loc); library(Biostrings, lib.loc = package_loc)
library(backports, lib.loc = package_loc); library(ggcorrplot, lib.loc = package_loc); library(ggpubr, lib = package_loc);
library(BiocGenerics, lib.loc = package_loc); library(dplyr, lib.loc = package_loc);
library(oligoClasses, lib.loc = package_loc); library(memoise, lib.loc = package_loc); library(pd.huex.1.0.st.v2, lib.loc = package_loc); library(oligo, lib.loc = package_loc, attach.required = TRUE)
# Part 2: Exploration
library(genefilter, lib = package_loc); library(limma, lib = package_loc); library(ggvenn, lib = package_loc)
library(multtest, lib = package_loc); library(annaffy, lib = package_loc)
# Part 3: Dimension Reduction
library(locfit, lib.loc = package_loc)
library(corrr, lib.loc = package_loc); library(idm, lib.loc = package_loc); library(irlba, lib.loc = package_loc)
library(PCAtools, lib.loc = package_loc); library(RMTstat, lib.loc = package_loc); library(biomaRt, lib.loc = package_loc); library(cowplot, lib.loc = package_loc); library(ggplotify, lib.loc = package_loc)
library(pROC, lib.loc = package_loc); library(withr, lib.loc = package_loc);
library(EFA.dimensions, lib.loc = package_loc)
library(corrplot, lib.loc = package_loc); library(factoextra, lib.loc = package_loc)
# Part 4: Analysis
library(e1071, lib.loc = package_loc); library(naivebayes, lib.loc = package_loc)
library(class, lib.loc = package_loc); library(gmodels, lib.loc = package_loc)
library(parallel, lib.loc = package_loc); library(doParallel, lib.loc = package_loc)
library(rpart, lib.loc = package_loc); library(rpart.plot, lib.loc = package_loc)
library(nnet, lib.loc = package_loc); library(rminer, lib.loc = package_loc)
library(randomForest, lib.loc = package_loc)
library(Hmisc, lib.loc = package_loc); library(caret, lib.loc = package_loc);
help(lda)
lda(diagnosis ~ ., data = trainset.multinomial)
BiocManager::install(
c(
"MASS"),
#force = TRUE,
dependencies = TRUE,
lib = package_loc
)
library(MASS, lib.loc = package_loc)
model.lda <- lda(diagnosis ~ .,
x = trainset.multinomial,
)
model.lda <- lda(diagnosis ~ .,
data = trainset.multinomial,
)
help(vif)
BiocManager::install(
c(
"car"),
#force = TRUE,
dependencies = TRUE,
lib = package_loc
)
library(corrplot, lib.loc = package_loc); library(factoextra, lib.loc = package_loc); library(car, lib.loc = package_loc)
help(diagnosis)
help(lm)
# Part 3.1.5: Check the collinearity of each predictor
model.gf <- lm(diagnosis ~ ., data = as.matrix(exprs(data.pp)[features.gf, ]))
# Part 3.1.5: Check the collinearity of each predictor
model.gf <- lm(diagnosis ~ ., data = as.data.frame(exprs(data.pp)[features.gf, ]))
# Part 3.1.5: Check the collinearity of each predictor
data.gf <- as.matrix(exprs(data.pp)[features.gf, ])
data.gf <- rbind(data.gf, diagnosis)
data.gf <- as.data.frame(t(data.gf))
data.gf$diagnosis <- factor(data.gf$diagnosis, ordered = FALSE)
model.gf <- lm(diagnosis ~ ., data = data.gf)
# Part 3.1.5: Check the collinearity of each predictor
data.gf <- as.matrix(exprs(data.pp)[features.gf, ])
data.gf <- rbind(data.gf, diagnosis.fact.named)
data.gf <- as.data.frame(t(data.gf))
data.gf$diagnosis <- factor(data.gf$diagnosis, ordered = FALSE)
model.gf <- lm(diagnosis ~ ., data = data.gf)
View(data.gf)
# Part 3.1.5: Check the collinearity of each predictor
data.gf <- as.matrix(exprs(data.pp)[features.gf, ])
data.gf <- rbind(data.gf, diagnosis.fact.named)
data.gf <- as.data.frame(t(data.gf))
data.gf$diagnosis.fact.named <- factor(data.gf$diagnosis.fact.named, ordered = FALSE)
model.gf <- lm(diagnosis.fact.named ~ ., data = data.gf)
View(data.gf)
# Part 3.1.5: Check the collinearity of each predictor
data.gf <- as.matrix(exprs(data.pp)[features.gf, ])
data.gf <- rbind(data.gf, diagnosis.fact.named)
data.gf <- as.data.frame(t(data.gf))
data.gf$diagnosis.fact.named <- factor(data.gf$diagnosis.fact.named)
model.gf <- lm(diagnosis.fact.named ~ ., data = data.gf)
# Part 3.1.5: Check the collinearity of each predictor
data.gf <- as.matrix(exprs(data.pp)[features.gf, ])
data.gf <- rbind(data.gf, diagnosis.binom)
data.gf <- as.data.frame(t(data.gf))
data.gf$diagnosis.binom <- factor(data.gf$diagnosis.binom)
model.gf <- lm(diagnosis.binom ~ ., data = data.gf)
# Part 3.1.5: Check the collinearity of each predictor
data.gf <- as.matrix(exprs(data.pp)[features.gf, ])
data.gf <- rbind(data.gf, diagnosis.binom)
data.gf <- as.data.frame(t(data.gf))
model.gf <- lm(diagnosis.binom ~ ., data = data.gf)
vif(model.gf)
vif(model.gf)[vif(model.gf) < 5]
colnames(vif(model.gf)[vif(model.gf) < 5])
View(vif(model.gf)[vif(model.gf) < 5])
rownames(vif(model.gf)[vif(model.gf) < 5])
a <- vif(model.gf)[vif(model.gf) < 5]
names(a)
as.str(names(a))
as.string(names(a))
# Part 3.1.5.1: Get only the factors with no to moderate collinearity (VIF <= 5)
features.gf <- names(vif(model.gf)[vif(model.gf) <= 5])
gsub("`", "", features.gf, fixed=T)
remove(data.gf, model.gf)
# Part 3.1.7: Create a correlation matrix across each features
features.gf.corr <- rcorr(as.matrix(t(exprs(data.pp)[features.gf, ])))
features.gf
features.gf <- gsub("`", "", features.gf, fixed = T)
# Part 3.1.7: Create a correlation matrix across each features
features.gf.corr <- rcorr(as.matrix(t(exprs(data.pp)[features.gf, ])))
corrplot(features.gf.corr$r)
# Step 3.2.11: Check for multicollinearity
data.pca <- as.matrix(exprs(data.pp)[features.pca, ])
data.pca <- rbind(data.pca, diagnosis.binom)
data.pca <- as.data.frame(t(data.pca))
model.pca <- lm(diagnosis.binom ~ ., data = data.pca)
# Part 3.1.5.1: Get only the factors with no to moderate collinearity (VIF <= 5)
features.pca <- names(vif(model.pca)[vif(model.pca) <= 5])
alias(model.pca)
alias(model.gf)
model.pca <- lm(diagnosis.binom ~ ., data = data.pca, singular.ok = TRUE)
# Part 3.1.5.1: Get only the factors with no to moderate collinearity (VIF <= 5)
features.pca <- names(vif(model.pca)[vif(model.pca) <= 5])
# Step 3.2.11: Check for multicollinearity
data.pca <- as.matrix(exprs(data.pp)[features.pca, ])
data.pca <- rbind(data.pca, diagnosis.binom)
data.pca <- as.data.frame(t(data.pca))
# Step 3.2.11: Check for multicollinearity
data.pca <- as.matrix(exprs(data.pp)[features.pca, ])
View(cor(data.pca))
View(cor(t(data.pca)))
a <- cor(t(data.pca))
a[which(a = 1)]
a[which(a == 1)]
a[which(a == 1), ]
a[a == 1, ]
# A which for multidimensional arrays.
# Mark van der Loo 16.09.2011
#
# A Array of booleans
# returns a sum(A) x length(dim(A)) array of multi-indices where A == TRUE
#
multi.which <- function(A){
if ( is.vector(A) ) return(which(A))
d <- dim(A)
T <- which(A) - 1
nd <- length(d)
t( sapply(T, function(t){
I <- integer(nd)
I[1] <- t %% d[1]
sapply(2:nd, function(j){
I[j] <<- (t %/% prod(d[1:(j-1)])) %% d[j]
})
I
}) + 1 )
}
a[multi.which(a == 1)]
multi.which(a)
multi.which(a == 1)
help("cor")
a[multi.which(a == 1)[]
a[multi.which(a == 1)[]
a[multi.which(a == 1)]
rownames(a)[14]
rownames(a)[13:14]
show_perfect_collinearity <- function(df) {
df <- cor(t(df))
multi.which(df == 1)
remove(df)
}
get_perfect_collinearity(data.pca)
show_perfect_collinearity <- function(df) {
df <- cor(t(df))
multi.which(df == 1)
remove(df)
}
get_perfect_collinearity(data.pca)
show_perfect_collinearity(data.pca)
show_perfect_collinearity <- function(df) {
df <- cor(t(df))
print(multi.which(df == 1))
remove(df)
}
show_perfect_collinearity <- function(df) {
df <- cor(t(df))
print(multi.which(df == 1))
remove(df)
}
# Step 3.2.11: Check for multicollinearity
data.pca <- as.matrix(exprs(data.pp)[features.pca, ])
show_perfect_collinearity(data.pca)
# Part 3.1.5: Check the collinearity of each predictor
data.gf <- as.matrix(exprs(data.pp)[features.gf, ])
show_perfect_collinearity(data.gf)
features.pca - "2908474"
# Step 3.2.11.1: From a correlation testing, features "2908474" "2908505" have perfect collinearity; remove them
features.pca <- features.pca[features.pca != c("2908474", "2908505")]
data.pca <- as.matrix(exprs(data.pp)[features.pca, ])
data.pca <- rbind(data.pca, diagnosis.binom)
data.pca <- as.data.frame(t(data.pca))
model.pca <- lm(diagnosis.binom ~ ., data = data.pca)
# Part 3.1.5.1: Get only the factors with no to moderate collinearity (VIF <= 5)
features.pca <- names(vif(model.pca)[vif(model.pca) <= 5])
features.pca <- gsub("`", "", features.pca, fixed = T)
remove(data.pca, model.pca)
# Part 3.2.10.2: Create a correlation matrix across each features
features.pca.corr <- rcorr(as.matrix(t(exprs(data.pp)[features.pca, ])))
corrplot(features.pca.corr$r)
remove(a)
remove(data.gf)
remove(data.pca.important)
remove(data.pca, model.pca)
# Step 3.3: Combine the features from Gene Filtering and PCA
features <- unique(append(features.gf, features.pca))
data.features <- as.matrix(exprs(data.pp)[features, ])
show_perfect_collinearity(data.features)
# Step 3.3.1: Look for perfect collinearity
data.features <- as.matrix(exprs(data.pp)[features, ])
show_perfect_collinearity(data.features)
# Step 3.3.2: Since there is no perfect collinearity, proceed
data.features <- as.matrix(exprs(data.pp)[features, ])
data.features <- rbind(data.features, diagnosis.binom)
data.features <- as.data.frame(t(data.features))
model.features <- lm(diagnosis.binom ~ ., data = data.features)
# Part 3.2.11.3: Get only the factors with no to moderate collinearity (VIF <= 5)
features.pca <- names(vif(model.features)[vif(model.features) <= 5])
features.pca <- gsub("`", "", features, fixed = T)
remove(data.features, model.features)
# Part 3.2.11.3: Get only the factors with no to moderate collinearity (VIF <= 5)
features.pca <- names(vif(model.pca)[vif(model.pca) <= 5])
# Step 3.2.10: For each of the principal component, get the variable with highest magnitude of eigenvalues
features.pca <- Reduce(intersect, List(features.pca.1, features.pca.2))
# Step 3.2.11: Check for multicollinearity
data.pca <- as.matrix(exprs(data.pp)[features.pca, ])
show_perfect_collinearity(data.pca)
show_perfect_collinearity(data.pca)
show_perfect_collinearity(data.pca)
# Step 3.2.11.1: From a correlation testing, features "2908474" "2908505" have perfect collinearity; remove them
features.pca <- features.pca[features.pca != c("2908474", "2908505")]
# Step 3.2.11.2: Proceed with getting VIF
data.pca <- as.matrix(exprs(data.pp)[features.pca, ])
data.pca <- rbind(data.pca, diagnosis.binom)
data.pca <- as.data.frame(t(data.pca))
model.pca <- lm(diagnosis.binom ~ ., data = data.pca)
# Part 3.2.11.3: Get only the factors with no to moderate collinearity (VIF <= 5)
features.pca <- names(vif(model.pca)[vif(model.pca) <= 5])
features.pca <- gsub("`", "", features.pca, fixed = T)
remove(data.pca, model.pca)
# Step 3.3: Combine the features from Gene Filtering and PCA
features <- unique(append(features.gf, features.pca))
# Step 3.3.1: Look for perfect collinearity
data.features <- as.matrix(exprs(data.pp)[features, ])
show_perfect_collinearity(data.features)
# Step 3.3.2: Since there is no perfect collinearity, proceed
data.features <- as.matrix(exprs(data.pp)[features, ])
data.features <- rbind(data.features, diagnosis.binom)
data.features <- as.data.frame(t(data.features))
model.features <- lm(diagnosis.binom ~ ., data = data.features)
# Part 3.2.11.3: Get only the factors with no to moderate collinearity (VIF <= 5)
features <- names(vif(model.features)[vif(model.features) <= 5])
features <- gsub("`", "", features, fixed = T)
remove(data.features, model.features)
# Part 3.2.14: Create a correlation matrix across each features
features.corr <- rcorr(as.matrix(t(exprs(data.pp)[features, ])))
corrplot(features.corr$r)
# Step 3.2.15: create Venn diagram and display all sets
ggvenn(list('Gene Filter' = features.gf,
'PCA' = features.pca
),
digits = 2
)
# Part 3.4: Splitting dataset
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
# Part 3.4.1: Get only the chosen features
data.multinomial <- as.matrix(exprs(data.pp)[features, ])
# Part 3.4.2: Replace the features into their transcript ID
# Part 3.4.2.1: Remove the unnecessary transcripts to save memory
ids.ensembl <- ids.ensembl[is.element(ids.ensembl$id_internal_huex, features), ]
#ids.ensembl[is.element(ids.ensembl$id_internal_huex, features), c('id_internal_huex', 'gene_id', 'transcript_id', 'exon_id')]
# Part 3.4.2.2: Create a list of gene names that will be used as rowname
get_probe_info <- function(probeId) {
return(ids.ensembl[ids.ensembl$id_internal_huex == probeid, ])
}
# Part 3.4.3: Insert the diagnosis factor in the dataframe
data.multinomial <- rbind(data.multinomial, diagnosis)
data.multinomial <- as.data.frame(t(data.multinomial))
data.multinomial$diagnosis <- factor(data.multinomial$diagnosis, ordered = FALSE)
# Part 3.4.4: Choose the index on which to choose as training
index.multinomial <- createDataPartition(data.multinomial$diagnosis, p = 0.75, list = F)
# Part 3.4.5: To make the training better, perform upsampling
trainset.multinomial <- data.multinomial[index.multinomial, ]
trainset.multinomial <- upSample(trainset.multinomial[, names(trainset.multinomial) %ni% c("diagnosis")], trainset.multinomial$diagnosis, yname = "diagnosis")
# Part 3.4.6: Correct the factors in testing
testset.multinomial <- data.multinomial[-index.multinomial, ]
testset.multinomial$diagnosis <- factor(testset.multinomial$diagnosis, ordered = FALSE)
# Step 3.4.7: Export
write.csv(data.multinomial, paste(datadir, "../Export/Chosen Dataset.csv", sep = ""), row.names = TRUE)
write.csv(ids.ensembl, paste(datadir, "../Export/Chosen Dataset Ensembl.csv", sep = ""), row.names = TRUE)
View(trainset.multinomial)
model.lda <- lda(diagnosis ~ .,
data = trainset.multinomial,
)
pred.model.lda <- predict(model.lda, newdata = testset.multinomial)
confMatrix.model.lda <- confusionMatrix(pred.model.lda, testset.multinomial$diagnosis)
pred.model.lda
help("lda")
pred.model.lda <- predict(model.lda, newdata = testset.multinomial, type = "class")
confMatrix.model.lda <- confusionMatrix(pred.model.lda, testset.multinomial$diagnosis)
pred.model.lda
