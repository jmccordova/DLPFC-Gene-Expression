# Part 1.5.4: Extracting probeset information
huex.probes <- read.csv(paste(probedir, 'HuEx-1_0-st-v2.na36.hg19.probeset.csv', sep = ''), comment.char = "#", header = TRUE)
# A which for multidimensional arrays.
# Mark van der Loo 16.09.2011
#
# A Array of booleans
# returns a sum(A) x length(dim(A)) array of multi-indices where A == TRUE
#
multi.which <- function(A){
if ( is.vector(A) ) return(which(A))
d <- dim(A)
T <- which(A) - 1
nd <- length(d)
t( sapply(T, function(t){
I <- integer(nd)
I[1] <- t %% d[1]
sapply(2:nd, function(j){
I[j] <<- (t %/% prod(d[1:(j-1)])) %% d[j]
})
I
}) + 1 )
}
show_perfect_collinearity <- function(df) {
df <- cor(t(df))
print(multi.which(df == 1))
remove(df)
}
# Part 2: Exploration
# Part 2.1: Get information about the dataset
slotNames(rawData)
str(rawData)
head(rawData)
# Part 2.2: Show how many gene expressions in the raw data
dim(exprs(rawData))   # 6553600     169
# Part 2.3: Platform used for gene information
annotation(rawData)   # "pd.huex.1.0.st.v2"
# Part 2.4: Show all the probes used for this dataset
unique(probeNames(rawData))
# Part 2.6: Turn probe-level information to gene-level data by normalization using RMA
# RMA - Robust Multi-Array Average (http://www.sthda.com/english/wiki/affymetrix-cel-files)
# Returns ExpressionSet
e <- rma(rawData)
# Part 2.7: Show details about the preprocessed values
str(e)
head(e)
# Part 2.8: Rename the columns of exprs(e)
colnames.e <- colnames(e)
for (i in 1:length(colnames.e)) {
colnames.e[i] <- strsplit(colnames.e[i], split = "_")[[1]][1]
}
colnames(e) <- colnames.e
data <- as.data.frame(exprs(e))
# Part 2.9: Check if expression set is normally distributed
p.vals <- rbind(
apply(data, 1, function(x) shapiro.test(as.matrix(x))$p.value),
apply(data, 1, function(x) kruskal.test(as.numeric(x) ~ as.factor(x))$p.value)
)
p.vals <- as.data.frame(p.vals)
row.names(p.vals) <- c("Normality: Shapiro-Wilk", "Population Equality: Kruskal-Wallis")
print(p.vals[1:10])
# Part 2.10: Create factors for each diagnosis
# Part 2.10.1: Replace string to integer classification
# Diagnosis from each sample in GEO
# CTL - 9, With Disorder - 1
diagnosis <- c(
# 299-309
9, 1, 1, 1, 1, 1, 1, 1, 1, 9, 1,
# 310-320
1, 1, 1, 9, 1, 1, 1, 1, 1, 9, 1,
# 321-331
1, 9, 1, 9, 1, 9, 9, 9, 1, 1, 9,
# 332-342
1, 9, 1, 1, 9, 1, 1, 9, 1, 1, 1,
# 343-353
9, 1, 9, 1, 9, 1, 1, 1, 1, 9, 1,
# 354-364
9, 9, 1, 9, 1, 1, 1, 9, 1, 9, 1,
# 365-375
1, 9, 9, 1, 9, 9, 9, 1, 1, 9, 1,
# 376-386
9, 1, 1, 9, 1, 1, 1, 1, 1, 9, 1,
# 387-397
9, 1, 1, 9, 1, 9, 1, 9, 1, 1, 1,
# 398-408
9, 9, 1, 1, 1, 9, 9, 9, 9, 1, 1,
# 409-419
9, 9, 1, 1, 9, 1, 1, 1, 1, 1, 1,
# 420-430
9, 1, 9, 1, 1, 1, 1, 1, 1, 1, 1,
# 431-441
1, 1, 1, 1, 1, 1, 1, 1, 9, 1, 1,
# 442-452
9, 9, 1, 9, 9, 1, 1, 9, 1, 9, 1,
# 453-463
1, 1, 9, 1, 9, 1, 9, 9, 9, 9, 9,
# 464-476
9, 9, 1, 1
)
diagnosis.binom <- diagnosis
diagnosis.binom[which(diagnosis.binom != 9)] <- 1
# Part 2.10.2: Create factors for each diagnosis
diagnosis.fact <- factor(diagnosis, ordered = FALSE)
diagnosis.binom.fact <- factor(diagnosis.binom, ordered = FALSE)
# Part 2.11: Alpha computation
# Perform Bonferroni correction
get_alpha <- function(alpha_orig, n) {
return(alpha_orig/n)
}
alpha <- 0.05
# Part 2.11: Check the normality of gene expressions per patient
# Part 2.11.1: Pick 5000 features; 5000 is the limit for shapiro.test()
testgenes.patient <- data[sample(rownames(data), 5000), ]
# Part 2.11.2: Perform tests on the column (patient)
p.vals.patient <- rbind(
apply(testgenes.patient, 2, function(x) shapiro.test(as.matrix(x))$p.value),
apply(testgenes.patient, 2, function(x) kruskal.test(as.numeric(x) ~ as.factor(x))$p.value)
)
row.names(p.vals.patient) <- c("Normality: Shapiro-Wilk", "Population Equality: Kruskal-Wallis")
p.vals.patient <- as.data.frame(p.vals.patient)
# Part 2.11.3: Check which of the patients have non-normal distribution of gene expressions
# Ho: The gene expressions are normally distributed.
# Ha: The gene expressions are not normally distributed.
print(colnames(p.vals.patient[1, which(p.vals.patient[1, ] < alpha)]))
# "In case the gene expression values over the patients are non-normally distributed one may want
# to subtract the median and divide by the MAD." (Krijnen)
data.pp <- data1 <- e[, colnames(p.vals.patient[1, which(p.vals.patient[1, ] < alpha)])]   # Get only those with
mads <- apply(exprs(data1), 2, mad)
meds <- apply(exprs(data1), 2, median)
dat <- sweep(exprs(data1), 2, meds)
exprs(data.pp) <- sweep(dat, 2, mads, FUN="/")
# Remove rawData variable to save space
rm(rawData, celFiles, data1)
BiocManager::install(
"Biobase",
force = TRUE,
dependencies = TRUE,
lib = package_loc
)
library(BiocManager, lib.loc = package_loc)
BiocManager::install(
"Biobase",
force = TRUE,
dependencies = TRUE,
lib = package_loc
)
library(Biobase, lib.loc = package_loc)
# Part 3: Data Preparation
# Part 3.1: Gene Filtering
# Part 3.1.1: Coefficient of Variation (cv)
# cv = 1 - standard deviation equals the mean, so that the experimental effect is small relative to the precision of measurement. If,
# cv < 0.2 - the mean is Â¯ve times larger than the standard deviation, so that both the experimental effect and the measurement precision are large.
cvval <- apply(exprs(data.pp), 1, function(x){sd(x)/abs(mean(x))})
features.gf.1 <- rownames(data[cvval < 0.2, ])
remove(cvval)
diagnosis
# Part 3.1.2: Normality and T-test
f1 <- function(x) (shapiro.test(x)$p.value > alpha)
f2 <- function(x) (t.test(x ~ diagnosis.binom.fact)$p.value < alpha)    # Ha - True differences between group means is zero
sel1 <- genefilter(exprs(e)[, which(diagnosis.fact != 9)], filterfun(f1))   # Select diagnosed patients
# Part 2: Exploration
library(genefilter, lib = package_loc); library(limma, lib = package_loc); library(ggvenn, lib = package_loc)
library(memoise, lib = package_loc)
library(multtest, lib = package_loc); library(annaffy, lib = package_loc)
library(GO.db, lib = package_loc);
library(pkgconfig, lib = package_loc);
library(multtest, lib = package_loc);  library(pkgconfig, lib = package_loc);  library(GO.db, lib = package_loc); library(annaffy, lib = package_loc)
sel1 <- genefilter(exprs(e)[, which(diagnosis.fact != 9)], filterfun(f1))   # Select diagnosed patients
# Part 2: Exploration
library(genefilter, lib = package_loc); library(memoise, lib = package_loc); library(limma, lib = package_loc); library(ggvenn, lib = package_loc)
library(withr, lib = package_loc);
library(ggplot2, lib = package_loc);
# Part 2: Exploration
library(withr, lib = package_loc); library(ggplot2, lib = package_loc);
library(genefilter, lib = package_loc); library(memoise, lib = package_loc); library(limma, lib = package_loc); library(ggvenn, lib = package_loc)
sel1 <- genefilter(exprs(e)[, which(diagnosis.fact != 9)], filterfun(f1))   # Select diagnosed patients
sel2 <- genefilter(exprs(e)[, which(diagnosis.fact == 9)], filterfun(f1))   # Select control patients
dbDisconnect()
dbDisconnect()
sel1 <- genefilter(exprs(e)[, which(diagnosis.fact != 9)], filterfun(f1))   # Select diagnosed patients
sel2 <- genefilter(exprs(e)[, which(diagnosis.fact == 9)], filterfun(f1))   # Select control patients
sel3 <- genefilter(exprs(e), filterfun(f2))
selected <- sel1 & sel2 & sel3
features.gf.2 <- rownames(data[selected, ])
remove(f1, f2, sel1, sel2, sel3, selected)
# Part 3.1.3: ANOVA
# Get all with diagnosis
diagnosis.fact.named <- diagnosis
diagnosis.fact.named[which(diagnosis.fact.named == 1)] <- "BPD"
# Part 3.1.3: ANOVA
# Get all with diagnosis
diagnosis.fact.named <- diagnosis
# Part 3.1.3: ANOVA
# Get all with diagnosis
diagnosis.fact.named <- diagnosis
diagnosis.fact.named[which(diagnosis.fact.named != 9)] <- "NOT-CTL"
diagnosis.fact.named[which(diagnosis.fact.named == 9)] <- "CTL"
data.pp.diagnosed <- data.pp[, which(diagnosis.fact.named != "CTL")]
panova <- apply(exprs(data.pp), 1, function(x) anova(lm(x ~ diagnosis.fact))$Pr[1])
features.gf.3 <- featureNames(data.pp)[panova < alpha]
remove(panova, data.pp.diagnosed)
# Part 3.1.4: Combine similar features
features.gf <- Reduce(intersect, List(features.gf.1, features.gf.2, features.gf.3))
# Part 3.1.5: Check the collinearity of each predictor
# Part 3.1.5.1: Look for perfect collinearity
data.gf <- as.matrix(exprs(data.pp)[features.gf, ])
show_perfect_collinearity(data.gf)
# Part 3.1.6: Since there's no perfect collinearity, proceed
data.gf <- as.matrix(exprs(data.pp)[features.gf, ])
data.gf <- rbind(data.gf, diagnosis.binom)
data.gf <- as.data.frame(t(data.gf))
model.gf <- lm(diagnosis.binom ~ ., data = data.gf)
# Part 3.1.6.1: Get only the factors with no to moderate collinearity (VIF <= 5)
features.gf <- names(vif(model.gf)[vif(model.gf) <= 5])
features.gf <- gsub("`", "", features.gf, fixed = T)
library(corrplot, lib.loc = package_loc); library(factoextra, lib.loc = package_loc); library(car, lib.loc = package_loc)
# Part 3: Dimension Reduction
library(locfit, lib.loc = package_loc)
library(corrr, lib.loc = package_loc); library(idm, lib.loc = package_loc); library(irlba, lib.loc = package_loc)
library(PCAtools, lib.loc = package_loc); library(RMTstat, lib.loc = package_loc); library(biomaRt, lib.loc = package_loc); library(cowplot, lib.loc = package_loc); library(ggplotify, lib.loc = package_loc)
library(PCAtools, lib.loc = package_loc); library(RMTstat, lib.loc = package_loc); library(rappdirs, lib.loc = package_loc); library(biomaRt, lib.loc = package_loc); library(cowplot, lib.loc = package_loc); library(ggplotify, lib.loc = package_loc)
library(pROC, lib.loc = package_loc); library(withr, lib.loc = package_loc);
library(EFA.dimensions, lib.loc = package_loc)
library(corrplot, lib.loc = package_loc); library(factoextra, lib.loc = package_loc); library(car, lib.loc = package_loc)
library(jsonlite, lib.loc = package_loc)
model.gf <- lm(diagnosis.binom ~ ., data = data.gf)
# Part 3.1.6.1: Get only the factors with no to moderate collinearity (VIF <= 5)
features.gf <- names(vif(model.gf)[vif(model.gf) <= 5])
features.gf <- gsub("`", "", features.gf, fixed = T)
remove(data.gf, model.gf)
# Part 3.1.6.2: Put to HTML the names of the features
atab <- aafTableAnn(features.gf, "pd.huex.1.0.st.v2", aaf.handler()[c(1:3,8:9,11:13)])
library(oligoClasses, lib.loc = package_loc); library(memoise, lib.loc = package_loc); library(pd.huex.1.0.st.v2, lib.loc = package_loc); library(oligo, lib.loc = package_loc, attach.required = TRUE)
# Part 3.1.6.2: Put to HTML the names of the features
atab <- aafTableAnn(features.gf, "pd.huex.1.0.st.v2", aaf.handler()[c(1:3,8:9,11:13)])
saveHTML(atab, file=paste(datadir, "../Export/Gene Filtering Probe Names.html", sep = ""))
# Part 3.1.7: Create a correlation matrix across each features
features.gf.corr <- rcorr(as.matrix(t(exprs(data.pp)[features.gf, ])))
library(jsonlite, lib.loc = package_loc); library(Hmisc, lib.loc = package_loc);
library(jsonlite, lib.loc = package_loc); library(backports, lib.loc = package_loc); library(Hmisc, lib.loc = package_loc);
# Part 3.1.7: Create a correlation matrix across each features
features.gf.corr <- rcorr(as.matrix(t(exprs(data.pp)[features.gf, ])))
corrplot(features.gf.corr$r)
# Part 3.1.8: create Venn diagram and display all sets
ggvenn(list('Coefficient of Variance' = features.gf.1,
'T-Test' = features.gf.2,
'ANOVA' = features.gf.3),
digits = 2
)
library(genefilter, lib = package_loc); library(memoise, lib = package_loc); library(limma, lib = package_loc); library(labeling, lib = package_loc); library(ggvenn, lib = package_loc)
# Part 3.1.8: create Venn diagram and display all sets
ggvenn(list('Coefficient of Variance' = features.gf.1,
'T-Test' = features.gf.2,
'ANOVA' = features.gf.3),
digits = 2
)
rlang::last_trace()
library(genefilter, lib = package_loc); library(memoise, lib = package_loc); library(limma, lib = package_loc); library(labeling, lib = package_loc); library(farver, lib = package_loc); library(ggvenn, lib = package_loc)
# Part 3.1.8: create Venn diagram and display all sets
ggvenn(list('Coefficient of Variance' = features.gf.1,
'T-Test' = features.gf.2,
'ANOVA' = features.gf.3),
digits = 2
)
# Part 3.2: Principal Component Analysis
# Step 3.2.1: Check for null values (If it returned more than 0, there is a null value)
print(colSums(is.na(exprs(data.pp)))[colSums(is.na(exprs(data.pp))) != 0])
# Step 3.2.3: Perform PCA
model.pca <- pca(exprs(data.pp), removeVar = 0.1)
# Step 3.2.4: Determine optimum number of PCs
# Step 3.2.4.1: Horn's method
pca.loadings.horn <- parallelPCA(exprs(data.pp))
# Step 3.2.4.2: Find the elbow point
pca.loadings.elbow <- findElbowPoint(model.pca$variance)
# Step 3.2.4.3: Get the PCs with that can explain at least 50% of the variability
print(paste('Number of PCs to consider:'))
print(paste('Horn:', pca.loadings.horn$n, '%Varation:', sum(model.pca$variance[1:pca.loadings.horn$n])))
print(paste('Elbow:', pca.loadings.elbow, '%Variation:', sum(model.pca$variance[1:pca.loadings.elbow])))
pca.loadings.min <- min(pca.loadings.horn$n, pca.loadings.elbow)
# Step 3.2.5: Get the loadings
model.pca.important <- as.data.frame(model.pca$loadings[, 1:pca.loadings.min])
# Step 3.2.6: Export
write.csv(model.pca.important, paste(datadir, "../Export/model.pca.important.csv", sep = ""), row.names = TRUE)
# Step 3.2.7: Visualize PCA
# Step 3.2.7.1: Scree plot to see where to cut the data
screeplot(model.pca,
components = getComponents(model.pca, 1:20),
vline = c(pca.loadings.horn$n, pca.loadings.elbow)) +
geom_label(aes(x = pca.loadings.horn$n + 1, y = 50,
label = 'Horn\'s', vjust = -1, size = 8)) +
geom_label(aes(x = pca.loadings.elbow + 1, y = 50,
label = 'Elbow method', vjust = -1, size = 8))
# Step 3.2.7: Visualize PCA
# Step 3.2.7.1: Scree plot to see where to cut the data
screeplot(model.pca,
components = getComponents(model.pca, 1:20),
vline = c(pca.loadings.horn$n, pca.loadings.elbow)) +
geom_label(aes(x = pca.loadings.horn$n + 1, y = 50,
label = 'Horn\'s', vjust = -1, size = 8)) +
geom_label(aes(x = pca.loadings.elbow + 1, y = 50,
label = 'Elbow method', vjust = -1, size = 8))
# Step 3.2.7.2: Pairs plot to compare one PC against another across all 5 PCs.
pairsplot(model.pca)
# Step 3.2.7.3: Biplot
biplot(model.pca, showLoadings = TRUE, labSize = 5, pointSize = 5, sizeLoadingsNames = 5)
# Step 3.2.7.4: Top 1%
plotloadings(model.pca,
rangeRetain = 0.01,
labSize = 4.0,
title = 'Loadings plot',
subtitle = 'PC1, PC2, PC3, PC4, PC5',
caption = 'Top 1% variables',
shape = 24,
col = c('limegreen', 'black', 'red3'),
drawConnectors = TRUE)
# Step 3.2.7.5: Top 10%
features.pca.1 <- plotloadings(model.pca,
components = getComponents(model.pca, 1:pca.loadings.min),
rangeRetain = 0.1,
labSize = 4.0,
absolute = FALSE,
title = 'Loadings plot',
subtitle = 'Misc PCs',
caption = 'Top 10% variables',
shape = 23, shapeSizeRange = c(1, 16),
col = c('white', 'pink'),
drawConnectors = FALSE)
# Step 3.2.7.6: Get the features of Top 10%
features.pca.1 <- features.pca.1$data$var
# Step 3.2.8: Transform to prcomp
#model.pca.prcomp <- BiocSingular::runPCA(t(exprs(e)), rank = pca.loadings.min, scale = TRUE)
model.pca.prcomp <- list(sdev = model.pca$sdev,
rotation = data.matrix(model.pca$loadings),
x = data.matrix(model.pca$rotated),
center = TRUE, scale = TRUE
)
class(model.pca.prcomp) <- 'prcomp'
# Step 3.2.9: Visualization of PCs
# Step 3.2.9.1: Graph of individuals. Individuals with a similar profile are grouped together.
fviz_pca_ind(model.pca.prcomp,
label="ind",
habillage=diagnosis.fact.named,
addEllipses=TRUE,
ellipse.level=0.95,
repel = TRUE
)
library(Biobase, lib.loc = package_loc); library(BiocSingular, lib.loc = package_loc); library(tzdb, lib.loc = package_loc); library(vroom, lib.loc = package_loc); library(readr, lib.loc = package_loc)
library(S4Vectors, lib.loc = package_loc); library(IRanges, lib.loc = package_loc); library(XVector, lib.loc = package_loc); library(GenomeInfoDb, lib.loc = package_loc); library(Biostrings, lib.loc = package_loc)
library(backports, lib.loc = package_loc); library(ggcorrplot, lib.loc = package_loc); library(ggpubr, lib = package_loc);
library(withr, lib = package_loc); library(backports, lib.loc = package_loc); library(ggcorrplot, lib.loc = package_loc); library(ggpubr, lib = package_loc);
library(BiocGenerics, lib.loc = package_loc); library(dplyr, lib.loc = package_loc);
library(oligoClasses, lib.loc = package_loc); library(memoise, lib.loc = package_loc); library(pd.huex.1.0.st.v2, lib.loc = package_loc); library(oligo, lib.loc = package_loc, attach.required = TRUE)
# Part 2: Exploration
library(withr, lib = package_loc); library(ggplot2, lib = package_loc);
library(genefilter, lib = package_loc); library(memoise, lib = package_loc); library(limma, lib = package_loc); library(labeling, lib = package_loc); library(farver, lib = package_loc); library(ggvenn, lib = package_loc)
library(multtest, lib = package_loc);  library(pkgconfig, lib = package_loc);  library(GO.db, lib = package_loc); library(annaffy, lib = package_loc)
# Part 3: Dimension Reduction
library(locfit, lib.loc = package_loc)
library(corrr, lib.loc = package_loc); library(idm, lib.loc = package_loc); library(irlba, lib.loc = package_loc)
library(PCAtools, lib.loc = package_loc); library(RMTstat, lib.loc = package_loc); library(rappdirs, lib.loc = package_loc); library(biomaRt, lib.loc = package_loc); library(cowplot, lib.loc = package_loc); library(ggplotify, lib.loc = package_loc)
library(pROC, lib.loc = package_loc); library(withr, lib.loc = package_loc);
library(EFA.dimensions, lib.loc = package_loc)
library(corrplot, lib.loc = package_loc); library(factoextra, lib.loc = package_loc); library(car, lib.loc = package_loc)
library(jsonlite, lib.loc = package_loc); library(backports, lib.loc = package_loc); library(Hmisc, lib.loc = package_loc);
# Part 4: Analysis
library(e1071, lib.loc = package_loc); library(naivebayes, lib.loc = package_loc)
library(class, lib.loc = package_loc); library(gmodels, lib.loc = package_loc)
library(parallel, lib.loc = package_loc); library(doParallel, lib.loc = package_loc)
library(rpart, lib.loc = package_loc); library(rpart.plot, lib.loc = package_loc)
library(nnet, lib.loc = package_loc); library(rminer, lib.loc = package_loc)
library(randomForest, lib.loc = package_loc)
library(MASS, lib.loc = package_loc); library(Metrics, lib.loc = package_loc);
library(caret, lib.loc = package_loc);
# Step 3.2.9: Visualization of PCs
# Step 3.2.9.1: Graph of individuals. Individuals with a similar profile are grouped together.
fviz_pca_ind(model.pca.prcomp,
label="ind",
habillage=diagnosis.fact.named,
addEllipses=TRUE,
ellipse.level=0.95,
repel = TRUE
)
model.pca.prcomp
# Step 3.2.9.2: Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.
features.pca.2 <- fviz_pca_var(model.pca.prcomp,
alpha.var="contrib",
col.var = "red3",
repel = TRUE,
select.var = list(contrib = length(features.pca.1)),
) + theme_minimal()
# Step 3.2.9.2: Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.
features.pca.2 <- fviz_pca_var(model.pca.prcomp,
alpha.var="contrib",
col.var = "red3",
repel = TRUE,
select.var = list(contrib = length(features.pca.1)),
) + theme_minimal()
features.pca.2 <- as.character(features.pca.2$data[, 'name'])
# Step 3.2.9.3: Biplot of individuals and variables
fviz_pca_biplot(model.pca.prcomp, repel = TRUE,
col.var = "#2E9FDF", # Variables color
col.ind = "#696969",  # Individuals color,
select.var = list(contrib = round(nrow(data.pp) * 0.01))
)
# Step 3.2.10: For each of the principal component, get the variable with highest magnitude of eigenvalues
features.pca <- Reduce(intersect, List(features.pca.1, features.pca.2))
# Step 3.2.11: Check for multicollinearity
data.pca <- as.matrix(exprs(data.pp)[features.pca, ])
show_perfect_collinearity(data.pca)
data.pca
View(data.pca)
# Step 3.2.11.1: From a correlation testing, features "2908474" "2908505" have perfect collinearity; remove them
features.pca <- features.pca[features.pca != c("2908474", "2908505")]
# Step 3.2.11.2: Proceed with getting VIF
data.pca <- as.matrix(exprs(data.pp)[features.pca, ])
data.pca <- rbind(data.pca, diagnosis.binom)
data.pca <- as.data.frame(t(data.pca))
model.pca <- lm(diagnosis.binom ~ ., data = data.pca)
# Part 3.2.11.3: Get only the factors with no to moderate collinearity (VIF <= 5)
features.pca <- names(vif(model.pca)[vif(model.pca) <= 5])
features.pca <- gsub("`", "", features.pca, fixed = T)
remove(data.pca, model.pca)
# Part 3.2.13: Put to HTML the names of the features
atab <- aafTableAnn(features.pca, "pd.huex.1.0.st.v2", aaf.handler()[c(1:3,8:9,11:13)])
saveHTML(atab, file=paste(datadir, "../Export/PCA Probe Names.html", sep = ""))
# Part 3.2.14: Create a correlation matrix across each features
features.pca.corr <- rcorr(as.matrix(t(exprs(data.pp)[features.pca, ])))
corrplot(features.pca.corr$r)
# Step 3.2.15: create Venn diagram and display all sets
ggvenn(list('PCATools' = features.pca.1,
'Factoextra' = features.pca.2
),
digits = 2
)
# Step 3.3: Combine the features from Gene Filtering and PCA
features <- unique(append(features.gf, features.pca))
# Step 3.3.1: Look for perfect collinearity
data.features <- as.matrix(exprs(data.pp)[features, ])
show_perfect_collinearity(data.features)
# Step 3.3.2: Since there is no perfect collinearity, proceed
data.features <- as.matrix(exprs(data.pp)[features, ])
data.features <- rbind(data.features, diagnosis.binom)
data.features <- as.data.frame(t(data.features))
model.features <- lm(diagnosis.binom ~ ., data = data.features)
# Part 3.3.3: Get only the factors with no to moderate collinearity (VIF <= 5)
features <- names(vif(model.features)[vif(model.features) <= 5])
features <- gsub("`", "", features, fixed = T)
remove(data.features, model.features)
# Part 3.3.4: Put to HTML the names of the features
atab <- aafTableAnn(features.pca, "pd.huex.1.0.st.v2", aaf.handler()[c(1:3,8:9,11:13)])
saveHTML(atab, file=paste(datadir, "../Export/GF + PCA Probe Names.html", sep = ""))
# Part 3.2.14: Create a correlation matrix across each features
features.corr <- rcorr(as.matrix(t(exprs(data.pp)[features, ])))
corrplot(features.corr$r)
# Step 3.2.15: create Venn diagram and display all sets
ggvenn(list('Gene Filter' = features.gf,
'PCA' = features.pca
),
digits = 2
)
# Part 3.4: Splitting dataset
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
# Part 3.4.1: Get only the chosen features
data.binomial <- as.matrix(exprs(data.pp)[features, ])
# Part 3.4.2: Keep only selected probes
huex.probes <- huex.probes[which(huex.probes$probeset_id %in% features), ]
# Part 3.4.2.1: Show probes not in the probeset annotation
features.missed <- features[which(features %ni% huex.probes$probeset_id)]
print(features.missed)
# Part 3.4.3: Insert the diagnosis factor in the dataframe
data.binomial <- rbind(data.binomial, diagnosis)
data.binomial <- as.data.frame(t(data.binomial))
data.binomial$diagnosis <- factor(data.binomial$diagnosis, ordered = FALSE)
# Part 3.4.4: Choose the index on which to choose as training
index.binomial <- createDataPartition(data.binomial$diagnosis, p = 0.75, list = F)
# Part 3.4.5: To make the training better, perform upsampling
trainset.binomial <- data.binomial[index.binomial, ]
trainset.binomial <- upSample(trainset.binomial[, names(trainset.binomial) %ni% c("diagnosis")], trainset.binomial$diagnosis, yname = "diagnosis")
# Part 3.4.6: Correct the factors in testing
testset.binomial <- data.binomial[-index.binomial, ]
testset.binomial$diagnosis <- factor(testset.binomial$diagnosis, ordered = FALSE)
# Step 3.4.7: Export
write.csv(data.binomial, paste(datadir, "../Export/Chosen Dataset.csv", sep = ""), row.names = TRUE)
write.csv(huex.probes, paste(datadir, "../Export/Chosen Probes.csv", sep = ""), row.names = TRUE)
remove(e, data)
results <- c()
for ( probe in features.missed ){
q <- paste(sep="","https://biodbnet-abcc.ncifcrf.gov/webServices/rest.php/biodbnetRestApi.json?method=db2db&input=affyid&inputValues=",probe,"&outputs=genesymbol&taxonId=9606&format=row")
results <- rbind(results,fromJSON(txt=q))
}
View(data.multinomial)
View(data.multinomial)
remove(data.multinomial, index.multinomial, testset.multinomial, trainset.multinomial, trainset.multinomial.cut, trainset.multinomial.preprocessed)
# Part 3.1.4: Alpha computation
# Perform Bonferroni correction
get_alpha <- function(alpha_orig, n) {
return(alpha_orig/n)
}
alpha <- 0.05
alpha <- get_alpha(alpha, ncol(data.binomial - 1))
alpha <- get_alpha(alpha, ncol(data.binomial) - 1)
alpha
# Part 4.1: Naive Bayes
set.seed(100)
model.nb <- train(x = trainset.binomial[, colnames(trainset.binomial) != "diagnosis"],
y = trainset.binomial$diagnosis,
method = "naive_bayes",
trControl = trainControl(method='cv', number=10)
)
pred.model.nb <- predict(model.nb, newdata = testset.binomial[, colnames(trainset.binomial) != "diagnosis"])
confMatrix.model.nb <- confusionMatrix(pred.model.nb, testset.binomial$diagnosis)
var.model.nb <- varImp(model.nb, useModel = TRUE, nonpara = TRUE, scale = TRUE)
