ntrees <- seq.int(3001, 4001,  by = 100)
} else {
ntrees <- c(rf.ntree)
}
mtries <- c(best_mtry)
for(ntree in ntrees) {
for(mtry in mtries) {
model.rf <- randomForest(x = trainset[, colnames(trainset) != "diagnosis"],
y = trainset$diagnosis,
ntree = ntree,
mtry = mtry,
oob.error=best_oob
)
pred.model.rf <- predict(model.rf, newdata = testset)
if (!tune) {
roc.model.rf <- auc(actual = testset$diagnosis,
predicted = predict(model.rf,
newdata = testset[, colnames(testset) != "diagnosis"],
type = "response"
)
)
}
confMatrix.model.rf <- confusionMatrix(pred.model.rf, testset$diagnosis)
if (tune) {
print(paste(ntree," and ", mtry))
print(confMatrix.model.rf)
}
var.model.rf <- varImp(model.rf, useModel = TRUE, nonpara = TRUE, scale = TRUE)
var.model.rf <- arrange(var.model.rf, desc(Overall))
}
}
if (!tune) {
return(list(model = model.rf, pred = pred.model.rf, confMatrix = confMatrix.model.rf, var = var.model.rf, roc = roc.model.rf))
}
} else {
models <- c("naive","ctree","cv.glmnet","rpart","kknn","ksvm","lssvm","mlp","mlpe", "randomForest","lda","multinom", "naiveBayes","xgboost", "SE")
if (method == "SOFT") {
models <- c(models, "SE")
}
model.auto <- rminer::fit(diagnosis ~ .,
data = trainset,
model = "auto",
fdebug = TRUE,
feature = "sabs",
scale = "inputs",
search = list(
search = mparheuristic(
model = models,
task = "class",
inputs = ncol(trainset)-1
),
smethod = "auto",
metric = "AUC",
convex = 0
)
)
pred.model.auto <- predict(model.auto, testset)
roc.model.auto <- round(mmetric(testset$diagnosis, pred.model.auto, metric="AUC"), 2)
importanceMethod <- "sens"
if (model.auto@model == "randomForest") {
importanceMethod <- "randomForest"
}
var.model.auto <- Importance(M = model.auto,
#method = importanceMethod,
#outindex = "diagnosis",
data = trainset
)
# Create dataframe that combines feature names and the variable importance
var.model.auto <- data.frame("feature" = colnames(trainset), "Overall" = var.model.auto$imp)
# Remove diagnosis row
var.model.auto <- head(var.model.auto, -1)
# Remove 0 importance
var.model.auto <- subset(var.model.auto, Overall != 0)
# Reorder from highest importance to least
var.model.auto <- var.model.auto[order(-var.model.auto$Overall),]
# show leaderboard:
cat("Models  by rank:", model.auto@mpar$LB$model, "\n")
cat("Validation values:", round(model.auto@mpar$LB$eval,4), "\n")
cat("Best model:", model.auto@model, "\n")
cat("AUC", "=", roc.model.auto, "\n")
return(list(model = model.auto, pred = pred.model.auto, confMatrix = rminer::mmetric(testset$diagnosis, pred.model.auto, "ALL"), var = var.model.auto, roc = roc.model.auto))
}
}
# Part 4.3.3: Combined Dataset
data.binomial <- createDataset(dataSource = data.pp, feature = features, probeset = huex.probes, filename = "PCA + GF")
sets <- buildTrainTest(data.binomial)
trainset.binomial <- sets$trainset
testset.binomial <- sets$testset
sets <- buildTrainTest(data.multinomial)
validationset.multinomial <- sets$validationset
remove(sets)
# Part 4.3.3.2: Perform analysis
# Part 4.3.3.2.1: Do ensemble of all learning methods
learn.features.auto <- perform_learning("auto", trainset.binomial, testset.binomial)
# Part 3.4: Splitting dataset
createDataset <- function(dataSource, features, probeset, filename) {
# Part 3.4.1: Get only the chosen features
data <- as.matrix(exprs(dataSource)[features, ])
# Part 3.4.2: Keep only selected probes
huex.probes <- huex.probes[which(probeset$probeset_id %in% features), ]
# Part 3.4.2.1: Show probes not in the probeset annotation
features.missed <- features[which(features %ni% probeset$probeset_id)]
print("Unannotated Features")
print(features.missed)
# Part 3.4.3: Insert the diagnosis factor in the dataframe
data <- rbind(data, diagnosis)
data <- as.data.frame(t(data))
data$diagnosis <- factor(data$diagnosis, ordered = FALSE)
# Step 3.4.4: Export
write.csv(data, paste(exportdir, exportsubdir, paste(filename, "Chosen Dataset.csv", sep = ""), sep = "/"), row.names = TRUE)
write.csv(huex.probes, paste(exportdir, exportsubdir, paste(filename, "Chosen Dataset.csv", sep = ""), sep = "/"), row.names = TRUE)
colnames(data) <- c(paste('feature_', colnames(data)[1:ncol(data)-1], sep = ''), 'diagnosis')
return(data)
}
# Part 3.5: Building dataset for training and testing
buildTrainTest <- function(dataSource) {
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
# Part 3.5.1: Choose the index on which to choose as training
index <- createDataPartition(dataSource$diagnosis, p = 0.75, list = F)
# Part 3.5.2: To make the training better, perform upsampling
trainset <- dataSource[index, ]
trainset <- upSample(trainset[, names(trainset) != "diagnosis"], trainset$diagnosis, yname = "diagnosis")
# Part 3.5.3: Correct the factors in testing
testset <- dataSource[-index, ]
testset$diagnosis <- factor(testset$diagnosis, ordered = FALSE)
return(list(trainset = trainset,
testset = testset))
}
buildTrainValidation <- function(dataSource) {
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
# Part 3.5.1: Choose the index on which to choose as training
index <- createDataPartition(dataSource$diagnosis, p = 0.75, list = F)
# Part 3.5.2: To make the training better, perform upsampling
trainset <- dataSource[index, ]
trainset <- upSample(trainset[, names(trainset) != "diagnosis"], trainset$diagnosis, yname = "diagnosis")
# Part 3.5.3: Correct the factors in testing
validationset <- dataSource[-index, ]
validationset$diagnosis <- factor(validationset$diagnosis, ordered = FALSE)
return(list(trainset = trainset,
validationset = validationset))
}
# Part 4.3.3: Combined Dataset
data.binomial <- createDataset(dataSource = data.pp, feature = features, probeset = huex.probes, filename = "PCA + GF")
sets <- buildTrainTest(data.binomial)
trainset.binomial <- sets$trainset
testset.binomial <- sets$testset
sets <- buildTrainTest(data.multinomial)
validationset.multinomial <- sets$validationset
remove(sets)
colnames(trainset.binomial)
# Part 4.3.3.2: Perform analysis
# Part 4.3.3.2.1: Do ensemble of all learning methods
learn.features.auto <- perform_learning("auto", trainset.binomial, testset.binomial)
# Part 4.3.3.2.2: Naive Bayes
learn.features.nb <- perform_learning("NB", trainset.binomial, testset.binomial)
learn.features.auto$var
nrow(learn.features.auto$var)
# Part 4.3.3.2.9: Soft voting from all the models
learn.features.soft <- perform_learning("SOFT", trainset.multinomial, testset.multinomial)
# Part 4.3.3.2.9: Soft voting from all the models
learn.features.soft <- perform_learning("SOFT", trainset.binomial, testset.binomial)
save.image("E:/jmcco/OneDrive - University of the Philippines/School/AY 2022-2023/2nd Sem/HI 299 Research Methods in Health Informatics/HI 299 Project/DLPFC-Gene-Expression/Binomial/Main.RData")
learn.features.soft$var
nrow(learn.features.soft$var)
matrix <- c(features,
c(sub("feature_", "", learn.features.auto$var$feature), rep("", length(features) - nrow(learn.features.auto$var))),
c(sub("feature_", "", learn.features.soft$var$feature), rep("", length(features) - nrow(learn.features.soft$var)))
)
matrix <- as.data.frame(matrix(matrix, ncol = 3))
colnames(matrix) <- c("Features", "Auto", "Soft")
# Part 5 - Ranking
exportsubdir <- "Step 5 - Ranking"
dir.create(paste(exportdir, exportsubdir, sep = "/"), recursive=TRUE)
write.csv(matrix, paste(exportdir, exportsubdir, "Rminer features.csv", sep = "/"), row.names = TRUE)
learn.features.soft$confMatrix
save.image("E:/jmcco/OneDrive - University of the Philippines/School/AY 2022-2023/2nd Sem/HI 299 Research Methods in Health Informatics/HI 299 Project/DLPFC-Gene-Expression/Binomial/Main.RData")
# Part 1: Setting up
exportsubdir <- "Part 1 - Setup"
library(BiocManager, lib.loc = package_loc)
library(Biobase, lib.loc = package_loc); library(BiocSingular, lib.loc = package_loc); library(tzdb, lib.loc = package_loc); library(vroom, lib.loc = package_loc); library(readr, lib.loc = package_loc)
library(S4Vectors, lib.loc = package_loc); library(IRanges, lib.loc = package_loc); library(XVector, lib.loc = package_loc); library(GenomeInfoDb, lib.loc = package_loc); library(Biostrings, lib.loc = package_loc)
library(withr, lib = package_loc); library(backports, lib.loc = package_loc); library(ggcorrplot, lib.loc = package_loc); library(ggpubr, lib = package_loc);
library(BiocGenerics, lib.loc = package_loc); library(dplyr, lib.loc = package_loc);
library(oligoClasses, lib.loc = package_loc); library(memoise, lib.loc = package_loc); library(pd.huex.1.0.st.v2, lib.loc = package_loc); library(oligo, lib.loc = package_loc, attach.required = TRUE)
# Part 2: Exploration
library(withr, lib = package_loc); library(ggplot2, lib = package_loc);
library(genefilter, lib = package_loc); library(memoise, lib = package_loc); library(limma, lib = package_loc); library(labeling, lib = package_loc); library(farver, lib = package_loc); library(ggvenn, lib = package_loc)
library(multtest, lib = package_loc);  library(pkgconfig, lib = package_loc);  library(GO.db, lib = package_loc); library(annaffy, lib = package_loc)
# Part 3: Dimension Reduction
library(locfit, lib.loc = package_loc)
library(corrr, lib.loc = package_loc); library(idm, lib.loc = package_loc); library(irlba, lib.loc = package_loc)
library(PCAtools, lib.loc = package_loc); library(psych, lib.loc = package_loc); library(RMTstat, lib.loc = package_loc); library(rappdirs, lib.loc = package_loc); library(biomaRt, lib.loc = package_loc); library(cowplot, lib.loc = package_loc); library(ggplotify, lib.loc = package_loc)
library(pROC, lib.loc = package_loc); library(withr, lib.loc = package_loc);
library(EFA.dimensions, lib.loc = package_loc)
library(corrplot, lib.loc = package_loc); library(factoextra, lib.loc = package_loc); library(car, lib.loc = package_loc)
library(jsonlite, lib.loc = package_loc); library(backports, lib.loc = package_loc); library(Hmisc, lib.loc = package_loc); library(glmnet, lib.loc = package_loc);
# Part 4: Analysis
library(e1071, lib.loc = package_loc); library(naivebayes, lib.loc = package_loc)
library(class, lib.loc = package_loc); library(gmodels, lib.loc = package_loc)
library(parallel, lib.loc = package_loc); library(doParallel, lib.loc = package_loc)
library(rpart, lib.loc = package_loc); library(rpart.plot, lib.loc = package_loc)
library(nnet, lib.loc = package_loc); library(rminer, lib.loc = package_loc); library(VGAM, lib.loc = package_loc)
library(randomForest, lib.loc = package_loc)
library(MASS, lib.loc = package_loc); library(Metrics, lib.loc = package_loc);
library(caret, lib.loc = package_loc); library(SuperLearner, lib.loc = package_loc); library(ROCR, lib.loc = package_loc);
# Part 5: Ranking
library(DescTools, lib.loc = package_loc); library(lmtest, lib.loc = package_loc);
# Part 4.3.3.2.9: Soft voting from all the models
learn.features.soft <- perform_learning("SOFT", trainset.binomial, testset.binomial)
ncol(data.binomial)
length(features)
save.image("E:/jmcco/OneDrive - University of the Philippines/School/AY 2022-2023/2nd Sem/HI 299 Research Methods in Health Informatics/HI 299 Project/DLPFC-Gene-Expression/Binomial/Main.RData")
learn.features.auto$model
learn.features.soft$var
nrow(learn.features.soft$var)
learn.features.soft$model
# Part 1: Setting up
exportsubdir <- "Part 1 - Setup"
library(BiocManager, lib.loc = package_loc)
library(Biobase, lib.loc = package_loc); library(BiocSingular, lib.loc = package_loc); library(tzdb, lib.loc = package_loc); library(vroom, lib.loc = package_loc); library(readr, lib.loc = package_loc)
library(S4Vectors, lib.loc = package_loc); library(IRanges, lib.loc = package_loc); library(XVector, lib.loc = package_loc); library(GenomeInfoDb, lib.loc = package_loc); library(Biostrings, lib.loc = package_loc)
library(withr, lib = package_loc); library(backports, lib.loc = package_loc); library(ggcorrplot, lib.loc = package_loc); library(ggpubr, lib = package_loc);
library(BiocGenerics, lib.loc = package_loc); library(dplyr, lib.loc = package_loc);
library(oligoClasses, lib.loc = package_loc); library(memoise, lib.loc = package_loc); library(pd.huex.1.0.st.v2, lib.loc = package_loc); library(oligo, lib.loc = package_loc, attach.required = TRUE)
# Part 2: Exploration
library(withr, lib = package_loc); library(ggplot2, lib = package_loc);
library(genefilter, lib = package_loc); library(memoise, lib = package_loc); library(limma, lib = package_loc); library(labeling, lib = package_loc); library(farver, lib = package_loc); library(ggvenn, lib = package_loc)
library(multtest, lib = package_loc);  library(pkgconfig, lib = package_loc);  library(GO.db, lib = package_loc); library(annaffy, lib = package_loc)
# Part 3: Dimension Reduction
library(locfit, lib.loc = package_loc)
library(corrr, lib.loc = package_loc); library(idm, lib.loc = package_loc); library(irlba, lib.loc = package_loc)
library(PCAtools, lib.loc = package_loc); library(psych, lib.loc = package_loc); library(RMTstat, lib.loc = package_loc); library(rappdirs, lib.loc = package_loc); library(biomaRt, lib.loc = package_loc); library(cowplot, lib.loc = package_loc); library(ggplotify, lib.loc = package_loc)
library(pROC, lib.loc = package_loc); library(withr, lib.loc = package_loc);
library(EFA.dimensions, lib.loc = package_loc)
library(corrplot, lib.loc = package_loc); library(factoextra, lib.loc = package_loc); library(car, lib.loc = package_loc)
library(jsonlite, lib.loc = package_loc); library(backports, lib.loc = package_loc); library(Hmisc, lib.loc = package_loc); library(glmnet, lib.loc = package_loc);
# Part 4: Analysis
library(e1071, lib.loc = package_loc); library(naivebayes, lib.loc = package_loc)
library(class, lib.loc = package_loc); library(gmodels, lib.loc = package_loc)
library(parallel, lib.loc = package_loc); library(doParallel, lib.loc = package_loc)
library(rpart, lib.loc = package_loc); library(rpart.plot, lib.loc = package_loc)
library(nnet, lib.loc = package_loc); library(rminer, lib.loc = package_loc); library(VGAM, lib.loc = package_loc)
library(randomForest, lib.loc = package_loc)
library(MASS, lib.loc = package_loc); library(Metrics, lib.loc = package_loc);
library(caret, lib.loc = package_loc); library(SuperLearner, lib.loc = package_loc); library(ROCR, lib.loc = package_loc);
# Part 5: Ranking
library(DescTools, lib.loc = package_loc); library(lmtest, lib.loc = package_loc);
help(rminer::Importance)
help('rminer::Importance')
help('Rminer::Importance')
help('Importance')
# Part 4.2: Create a function for each ML
perform_learning <- function(method, trainset, testset,
svm.kernel = NULL,
svm.cost = NULL,
rf.ntree = NULL,
rf.mtry = NULL,
export.filename = NULL,
tune = FALSE) {
if (method == "NB") {
# Part 4.1: Naive Bayes
set.seed(100)
model.nb <- train(x = trainset[, colnames(trainset) != "diagnosis"],
y = trainset$diagnosis,
method = "naive_bayes",
trControl = trainControl(method='cv', number=10)
)
pred.model.nb <- predict(model.nb, newdata = testset[, colnames(trainset) != "diagnosis"])
roc.model.nb <- auc(actual = testset$diagnosis,
predicted = predict(model.nb,
newdata = testset[, colnames(testset) != "diagnosis"],
type = "raw"
)
)
confMatrix.model.nb <- confusionMatrix(pred.model.nb, testset$diagnosis)
#var.model.nb <- varImp(model.nb, useModel = TRUE, nonpara = TRUE, scale = TRUE)
var.model.nb <- varImp(model.nb, scale = FALSE)
return(list(model = model.nb, pred = pred.model.nb, confMatrix = confMatrix.model.nb, var = var.model.nb, roc = roc.model.nb))
} else if (method == "KNN") {
# Part 4.2: K Nearest Neighbors
set.seed(100)
trControl.knn <- trainControl(method='repeatedcv', number = 3, allowParallel = TRUE)
trainset.preprocessed <- preProcess(trainset[, colnames(trainset) != "diagnosis"])
model.knn <- train(x = trainset[, colnames(trainset) != "diagnosis"],
y = trainset$diagnosis,
method = "knn",
trControl = trControl.knn,
tuneLength = 20
)
pred.model.knn <- predict(model.knn, newdata = testset)
roc.model.knn <- auc(actual = testset$diagnosis,
predicted = predict(model.knn,
newdata = testset[, colnames(testset) != "diagnosis"],
type = "raw"
)
)
confMatrix.model.knn <- confusionMatrix(pred.model.knn, testset$diagnosis)
#var.model.knn <- varImp(model.knn, useModel = TRUE, nonpara = TRUE, scale = TRUE)
var.model.knn <- varImp(model.knn, scale = FALSE)
return(list(model = model.knn, pred = pred.model.knn, confMatrix = confMatrix.model.knn, var = var.model.knn, roc = roc.model.knn))
} else if (method == "DT") {
# Part 4.3: Decision Tree
# Part 4.3.1: Using rpart
model.dt <- rpart(formula = diagnosis ~ .,
data = trainset,
method = "class",
control = rpart.control(minsplit=2, minbucket = 1, cp = 0.001)
)
#var.model.dt<- varImp(model.dt, useModel = TRUE, nonpara = TRUE, scale = TRUE)
var.model.dt<- varImp(model.dt, scale = FALSE)
arrange(var.model.dt, desc(Overall))
rpart.plot(model.dt)
pdf(export.filename)
prp(model.dt, extra=104)
dev.off()
# Part 4.3.2: Using train
## 10-fold CV
## repeated ten times
trControl.dt <- trainControl(
method = "repeatedcv",
number = 5,
repeats = 10)
model.dt <- train(x = trainset[, colnames(trainset) != "diagnosis"],
y = trainset$diagnosis,
method = "rpart2",
trControl = trControl.dt
)
pred.model.dt <- predict(model.dt, newdata = testset)
roc.model.dt <- auc(actual = testset$diagnosis,
predicted = predict(model.dt,
newdata = testset[, colnames(testset) != "diagnosis"],
type = "raw"
)
)
confMatrix.model.dt <- confusionMatrix(pred.model.dt, testset$diagnosis)
return(list(model = model.dt, pred = pred.model.dt, confMatrix = confMatrix.model.dt, var = var.model.dt, roc = roc.model.dt))
} else if (method == "SVM") {
# Part 4.4: SVM
# For SVM and random forest, cut the dataset to 10% of the dataset to make processing quicker
#trainset.cut <- trainset[sample(x = 1:nrow(trainset), size = nrow(trainset) * .10, replace = TRUE), colnames(trainset)]
#trainset.cut <- upSample(x = trainset.cut[, colnames(trainset.cut) %ni% "ADDEPEV3"], yname = "ADDEPEV3", y = trainset.cut$ADDEPEV3)
model.svm <- rminer::fit(diagnosis ~ .,
data = trainset,
model = "ksvm",
kpar = "automatic",
task = "class",
search = list(search = mparheuristic("ksvm", n = 5))
)
pred.model.svm <- predict(model.svm, newdata = testset)
confMatrix.model.svm <- confusionMatrix(pred.model.svm, testset$diagnosis)
if (tune) {
print(paste(kernel," @ ", cost))
print(confMatrix.model.svm)
}
var.model.svm <- Importance(model.svm, data = trainset, method = "DSA", outindex = "diagnosis")
var.model.svm <- data.frame("feature" = colnames(var.model.svm$data), "Overall" = var.model.svm$imp)
var.model.svm <- head(var.model.svm, -1)
var.model.svm <- var.model.svm[order(-var.model.svm$Overall),]
if (!tune) {
roc.model.svm <- auc(actual = testset$diagnosis,
predicted = predict(model.svm,
newdata = testset[, colnames(testset) != "diagnosis"],
type = "raw"
)
)
}
if (!tune) {
return(list(model = model.svm, pred = pred.model.svm, confMatrix = confMatrix.model.svm, var = var.model.svm, roc = roc.model.svm))
}
} else if (method == "LOG") {
model.logit <- multinom(diagnosis ~ .,
data = trainset)
pred.model.logit <- predict(model.logit, newdata = testset[, colnames(testset) != "diagnosis"], type = "class")
roc.model.logit <- multiclass.roc(response = testset$diagnosis,
predictor = predict(model.logit,
newdata = testset[, colnames(testset) != "diagnosis"],
type = "prob"),
percent = TRUE)
roc.model.logit <- auc(actual = testset$diagnosis,
predicted = predict(model.logit,
newdata = testset[, colnames(testset) != "diagnosis"],
type = "probs"
)
)
confMatrix.model.logit <- confusionMatrix(pred.model.logit, testset$diagnosis)
#var.model.logit <- varImp(model.logit, useModel = TRUE, nonpara = TRUE, scale = TRUE)
var.model.logit <- varImp(model.logit, scale = FALSE)
var.model.logit <- data.frame("feature" = rownames(var.model.logit), "Overall" = var.model.logit$Overall)
var.model.logit <- var.model.logit[order(-var.model.logit$Overall), ]
#summary(model.logit)
#View(cbind("coeff" = coef(model.logit), "odds ratio" = (exp(coef(model.logit)) - 1) * 100)) # Odds ratio
return(list(model = model.logit, pred = pred.model.logit, confMatrix = confMatrix.model.logit, var = var.model.logit, roc = roc.model.logit))
} else if (method == "DA") {
# Part 4.6: Discriminant Analysis
trControl.lda <- trainControl(classProbs = TRUE)
levels(trainset$diagnosis)[match("1",levels(trainset$diagnosis))] <- "DIS"
levels(trainset$diagnosis)[match("9",levels(trainset$diagnosis))] <- "CTL"
model.lda <- train(diagnosis ~ .,
data = trainset,
method = "lda",
trControl = trControl.lda)
levels(testset$diagnosis)[match("1",levels(testset$diagnosis))] <- "DIS"
levels(testset$diagnosis)[match("9",levels(testset$diagnosis))] <- "CTL"
var.model.lda <- varImp(model.lda, useModel = TRUE, nonpara = TRUE, scale = TRUE)
model.lda <- lda(diagnosis ~ .,
data = trainset,
)
pred.model.lda <- predict(model.lda, newdata = testset, type = "response")
roc.model.lda <- auc(testset$diagnosis, pred.model.lda$class)
confMatrix.model.lda <- confusionMatrix(pred.model.lda$class, testset$diagnosis)
#return(list(model = model.lda, pred = pred.model.lda, confMatrix = confMatrix.model.lda, var = var.model.lda))
return(list(model = model.lda, pred = pred.model.lda, confMatrix = confMatrix.model.lda, var = var.model.lda, roc = roc.model.lda))
} else if (method == "RF") {
# Part 4.7: Random Forest
set.seed(100)
# tuneRF
invisible(capture.output(fgl.res <- tuneRF(x = trainset[, colnames(trainset) != "diagnosis"],
y = trainset$diagnosis,
stepFactor=1.5)
)
)
# choose the best mtry based on the lowest OOB error
best_mtry <- fgl.res[fgl.res[, 2] == min(fgl.res[, 2]), 1]
# choose the lowest OOB error
best_oob  <- fgl.res[fgl.res[, 2] == min(fgl.res[, 2]), 2]
if (tune) {
ntrees <- seq.int(3001, 4001,  by = 100)
} else {
ntrees <- c(rf.ntree)
}
mtries <- c(best_mtry)
for(ntree in ntrees) {
for(mtry in mtries) {
model.rf <- randomForest(x = trainset[, colnames(trainset) != "diagnosis"],
y = trainset$diagnosis,
ntree = ntree,
mtry = mtry,
oob.error=best_oob
)
pred.model.rf <- predict(model.rf, newdata = testset)
if (!tune) {
roc.model.rf <- auc(actual = testset$diagnosis,
predicted = predict(model.rf,
newdata = testset[, colnames(testset) != "diagnosis"],
type = "response"
)
)
}
confMatrix.model.rf <- confusionMatrix(pred.model.rf, testset$diagnosis)
if (tune) {
print(paste(ntree," and ", mtry))
print(confMatrix.model.rf)
}
var.model.rf <- varImp(model.rf, useModel = TRUE, nonpara = TRUE, scale = TRUE)
var.model.rf <- arrange(var.model.rf, desc(Overall))
}
}
if (!tune) {
return(list(model = model.rf, pred = pred.model.rf, confMatrix = confMatrix.model.rf, var = var.model.rf, roc = roc.model.rf))
}
} else {
models <- c("naive","ctree","cv.glmnet","rpart","kknn","ksvm","lssvm","mlp","mlpe", "randomForest","lda","multinom", "naiveBayes","xgboost", "SE")
if (method == "SOFT") {
models <- c(models, "SE")
}
model.auto <- rminer::fit(diagnosis ~ .,
data = trainset,
model = "auto",
fdebug = TRUE,
feature = "sabs",
scale = "inputs",
search = list(
search = mparheuristic(
model = models,
task = "class",
inputs = ncol(trainset)-1
),
smethod = "auto",
metric = "AUC",
convex = 0
)
)
pred.model.auto <- predict(model.auto, testset)
roc.model.auto <- round(mmetric(testset$diagnosis, pred.model.auto, metric="AUC"), 2)
importanceMethod <- "sens"
if (model.auto@model == "randomForest") {
importanceMethod <- "randomForest"
}
var.model.auto <- Importance(M = model.auto,
#method = importanceMethod,
#outindex = "diagnosis",
data = trainset
)
print(var.model.auto)
# Create dataframe that combines feature names and the variable importance
var.model.auto <- data.frame("feature" = colnames(trainset), "Overall" = var.model.auto$imp)
# Remove diagnosis row
var.model.auto <- head(var.model.auto, -1)
# Remove 0 importance
var.model.auto <- subset(var.model.auto, Overall != 0)
# Reorder from highest importance to least
var.model.auto <- var.model.auto[order(-var.model.auto$Overall),]
# show leaderboard:
cat("Models  by rank:", model.auto@mpar$LB$model, "\n")
cat("Validation values:", round(model.auto@mpar$LB$eval,4), "\n")
cat("Best model:", model.auto@model, "\n")
cat("AUC", "=", roc.model.auto, "\n")
return(list(model = model.auto, pred = pred.model.auto, confMatrix = rminer::mmetric(testset$diagnosis, pred.model.auto, "ALL"), var = var.model.auto, roc = roc.model.auto))
}
}
